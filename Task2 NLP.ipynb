{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Labeled Customer Reviews to Make Binary Sentiment Predictions With NLP and Recurrent Neural Networks\n",
    "\n",
    "Max Sydow.  Jul 2020.  WGU. Master of Science, Data Analytics Program. Course D213, Performance Assessment II.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question and Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can natural language processing be used to predict negative or positive customer sentiment based on their verbal or writtent reviews?  This would give companies a better idea of customer perceptions and may form a basis for finding areas of improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written customer reviews from 3 sources: Amazon, IMDB, and Yelp will be examined.  Each review in these data sets are assigned a sentiment rating or 1 for positive, or 0 for negative.  There are thusly 2 columns with each row describing a single review and sentiment pair. The number of words in each review may vary greatly, and so a model to make such predictions needs to be able to handle a wide range of input sizes.  A neural network (NN) is such a model.  Some preproccessing is required in order for it to work right though. The inputs to be numeric, so a method of assigning words to numbers needs to be employed.  Counting similar words and even the same words typed using different case can affect predictions, so can the appearance of numbers or special characters.  An algorithm can only work with what is fed to it, after all.  With that in mind, meaningless words like 'an' or 'the' would not add much value.  Maximum number of processed words and average word length are other factors that will come into consideration when building the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's natural language toolkit (nltk) will be useful for simplifying vocabulary.  Sklearn has some useful functions for representing words and letters as numbers and splitting data into training and testing sets.  Pandas and Numpy are involved with data handling, while matplotlib allows for graphing.   The Tensorflow and Keras packages allow nueral networks to be constructed.  In particular, Keras allows for high-level api creation of NNs using layers.  Since a NN can be represented as sets of interconnected nodes divided into layers it makes sense to be able create them that way.  Each connection between nodes applies a weight to the node value which is transformed via a mathematical function.  Predictions are made by the changes that data goes through under these transformations.  Along the way performance metrics are honed and optimized in both directions throughout the network.  A NN that can do this is called Recurrent, or RNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import preprocessing\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout, LSTM, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxgs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\maxgs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\maxgs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading text files into dataframes and naming columns\n",
    "amzn_df = pd.read_csv('C:/Users/maxgs/MSDA/D213/amazon_cells_labelled.txt', delimiter='\\t', header=None)\n",
    "amzn_df.columns = [\"review\", \"sentiment\"]\n",
    "imdb_df = pd.read_csv('C:/Users/maxgs/MSDA/D213/imdb_labelled.txt', delimiter='\\t', header=None)\n",
    "imdb_df.columns = [\"review\", \"sentiment\"]\n",
    "yelp_df = pd.read_csv('C:/Users/maxgs/MSDA/D213/yelp_labelled.txt', delimiter='\\t', header=None)\n",
    "yelp_df.columns = [\"review\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique sentiments, amazon:  [0 1]\n",
      "unique sentiments, imdb:  [0 1]\n",
      "unique sentiments, yelp:  [1 0]\n"
     ]
    }
   ],
   "source": [
    "# verify sentiment label values\n",
    "print('unique sentiments, amazon: ', amzn_df['sentiment'].unique())\n",
    "print('unique sentiments, imdb: ', imdb_df['sentiment'].unique())\n",
    "print('unique sentiments, yelp: ', yelp_df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of amzn:  (1000, 2)\n",
      "dimensions of imdb:  (748, 2)\n",
      "dimensions of yelp:  (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('dimensions of amzn: ', amzn_df.shape)\n",
    "print('dimensions of imdb: ', imdb_df.shape)\n",
    "print('dimensions of yelp: ', yelp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all 3 dataframes\n",
    "\n",
    "sent_df = pd.concat([amzn_df, imdb_df, yelp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of combined dataframe\n",
    "sent_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2748.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.504367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  2748.000000\n",
       "mean      0.504367\n",
       "std       0.500072\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here in the US unless I go by a converter.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have to jiggle the plug to get it to line up right to get decent volume.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Needless to say, I wasted my money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What a waste of money and time!.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            review  \\\n",
       "0                               So there is no way for me to plug it in here in the US unless I go by a converter.   \n",
       "1                                                                                      Good case, Excellent value.   \n",
       "2                                                                                           Great for the jawbone.   \n",
       "3                                  Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!   \n",
       "4                                                                                                The mic is great.   \n",
       "5                                       I have to jiggle the plug to get it to line up right to get decent volume.   \n",
       "6  If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.   \n",
       "7                                                                      If you are Razr owner...you must have this!   \n",
       "8                                                                              Needless to say, I wasted my money.   \n",
       "9                                                                                 What a waste of money and time!.   \n",
       "\n",
       "   sentiment  \n",
       "0          0  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          1  \n",
       "5          0  \n",
       "6          0  \n",
       "7          1  \n",
       "8          0  \n",
       "9          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "sent_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentiments to integer\n",
    "sent_df['sentiment'] = sent_df['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case any values in the review column are not string\n",
    "for i in range(0, len(sent_df)-1):\n",
    "    if type(sent_df.iloc[i]['review'])!=str:\n",
    "        sent_df.iloc[i]['review'] = str(df.iloc[i]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'o', ' ', 't', 'h', 'e', 'r', 'i', 's', 'n', 'w', 'a', 'y', 'f', 'm', 'p', 'l', 'u', 'g', 'U', 'I', 'b', 'c', 'v', '.', 'G', 'd', ',', 'E', 'x', 'j', 'T', '4', '5', 'M', 'A', 'J', 'O', 'R', 'P', 'B', 'L', '!', 'z', 'N', 'W', 'q', 'H', '+', 'V', '\"', 'Y', 'D', 'F', 'k', \"'\", 'K', 'C', '/', '7', '3', '6', '8', '0', '2', '?', 'Z', '-', '1', ':', ')', '(', 'Q', '&', '$', '*', ';', 'X', '%', '9', '#', '[', ']', '\\x96', '\\t', '\\n', 'é', '\\x85', 'å', '\\x97', 'ê']\n"
     ]
    }
   ],
   "source": [
    "# look at unique characters \n",
    "rev = sent_df['review']\n",
    "list_chars = []\n",
    "\n",
    "for comment in rev:\n",
    "    #print(comment)\n",
    "    for char in comment:\n",
    "        #print(char)\n",
    "        if char not in list_chars:\n",
    "            list_chars.append(char)\n",
    "print(list_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  5272\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sent_df['review'])\n",
    "print('Vocab size: ', len(tokenizer.word_index) +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    So there is no way for me to plug it in here in the US unless I go by a converter \n",
       "1                                                           Good case, Excellent value \n",
       "2                                                                Great for the jawbone \n",
       "3       Tied to charger for conversations lasting more than 45 minutes MAJOR PROBLEMS!!\n",
       "4                                                                     The mic is great \n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df['review'] = sent_df['review'].str.replace('.',' ')\n",
    "sent_df['review'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    so there is no way for me to plug it in here in the us unless i go by a converter \n",
       "1                                                           good case  excellent value \n",
       "2                                                                great for the jawbone \n",
       "3        tied to charger for conversations lasting more than 45 minutes major problems \n",
       "4                                                                     the mic is great \n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use regex to replace non-alphabetic characters\n",
    "sent_df['review'] = sent_df['review'].str.replace(r'[^\\w\\s]+',' ')\n",
    "\n",
    "# make all chars lowercase\n",
    "sent_df['review'] = sent_df['review'].str.lower()\n",
    "\n",
    "sent_df['review'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    so there is no way for me to plug it in here in the us unless i go by a converter \n",
       "1                                                           good case  excellent value \n",
       "2                                                                great for the jawbone \n",
       "3          tied to charger for conversations lasting more than  minutes major problems \n",
       "4                                                                     the mic is great \n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still some numbers to get rid of\n",
    "sent_df['review'] = sent_df['review'].str.replace(r'[\\d]+','')\n",
    "sent_df['review'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    so there is no way for me to plug it in here in the us unless i go by a converter \n",
       "1                                                            good case excellent value \n",
       "2                                                                great for the jawbone \n",
       "3           tied to charger for conversations lasting more than minutes major problems \n",
       "4                                                                     the mic is great \n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce to single space between words\n",
    "sent_df['review'] = sent_df['review'].str.replace(r'[\\s]+',' ')\n",
    "sent_df['review'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'o', ' ', 't', 'h', 'e', 'r', 'i', 'n', 'w', 'a', 'y', 'f', 'm', 'p', 'l', 'u', 'g', 'b', 'c', 'v', 'd', 'x', 'j', 'z', 'q', 'k', 'é', 'å', 'ê']\n"
     ]
    }
   ],
   "source": [
    "rev_abc = sent_df['review']\n",
    "list_abc = []\n",
    "for comment in rev_abc:\n",
    "    for char in comment:\n",
    "        if char not in list_abc:\n",
    "            list_abc.append(char)\n",
    "print(list_abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still some special chars to get rid of\n",
    "for char in ['é','å','ê']:\n",
    "    sent_df['review'] = sent_df['review'].str.replace(char, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'o', ' ', 't', 'h', 'e', 'r', 'i', 'n', 'w', 'a', 'y', 'f', 'm', 'p', 'l', 'u', 'g', 'b', 'c', 'v', 'd', 'x', 'j', 'z', 'q', 'k']\n"
     ]
    }
   ],
   "source": [
    "# much better\n",
    "rev_abc_2 = sent_df['review']\n",
    "list_abc_2 = []\n",
    "for comment in rev_abc_2:\n",
    "    for char in comment:\n",
    "        if char not in list_abc_2:\n",
    "            list_abc_2.append(char)\n",
    "print(list_abc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "sent_df['review'] = sent_df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sent_df['review'] = sent_df['review'].apply(lambda x: wordnet_lemmatizer.lemmatize(x, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              way plug us unless go converter\n",
       "1                                    good case excellent value\n",
       "2                                                great jawbone\n",
       "3    tied charger conversations lasting minutes major problems\n",
       "4                                                    mic great\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df['review'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocab size:  4958\n"
     ]
    }
   ],
   "source": [
    "# no stop no lem vocabulary size\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sent_df['review'])\n",
    "vocab_size = len(tokenizer.word_index) +1\n",
    "print('New vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding length:  8\n"
     ]
    }
   ],
   "source": [
    "# word embedding length\n",
    "emb = int(round(np.sqrt(np.sqrt(vocab_size)),0))\n",
    "print(\"Word embedding length: \", emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max review length:  677\n",
      "Vocab size:  4958\n"
     ]
    }
   ],
   "source": [
    " # sequence length - longest sentence\n",
    "rev_list = sent_df['review'].tolist()\n",
    "vocab = []\n",
    "max_rev_len = 0\n",
    "\n",
    "for rev in rev_list:\n",
    "    rev_len = len(rev.split(' '))\n",
    "    if rev_len > max_rev_len:\n",
    "        max_rev_len = rev_len\n",
    "    \n",
    "    for word in rev.split(' '):\n",
    "        if not word in vocab:\n",
    "            vocab.append(word)\n",
    "        \n",
    "print(\"Max review length: \", max_rev_len)\n",
    "print(\"Vocab size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum length of any review is 677 words.  If the input rows to a model included less than this, some words would be cut out thereby affecting the results.  Rows with smaller lengths can be padded to match the maximum length.  First, each word can be tokenized so that it has a unique numerical representaion.  In this sense the vocabulary becomes an indexed dictionary of words.  The Tokenizer from tensorflow.keras accomplished this already, and was used to compute the vocabulary length above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': 1, 'great': 2, 'movie': 3, 'phone': 4, 'film': 5, 'one': 6, 'like': 7, 'food': 8, 'place': 9, 'time': 10, 'service': 11, 'bad': 12, 'really': 13, 'well': 14, 'would': 15, 'best': 16, 'even': 17, 'ever': 18, 'also': 19, 'back': 20, 'quality': 21, 'go': 22, 'love': 23, 'made': 24, 'work': 25, 'product': 26, 'get': 27, 'excellent': 28, 'could': 29, 'nice': 30, 'better': 31, 'recommend': 32, 'never': 33, 'works': 34, 'sound': 35, 'much': 36, 'use': 37, 'headset': 38, 'think': 39, 'first': 40, 'battery': 41, 'way': 42, 'pretty': 43, 'see': 44, 'acting': 45, 'make': 46, 'worst': 47, 'still': 48, 'got': 49, 'say': 50, 'everything': 51, 'enough': 52, 'two': 53, 'ear': 54, 'little': 55, 'every': 56, 'characters': 57, 'right': 58, 'going': 59, 'thing': 60, 'price': 61, 'amazing': 62, 'minutes': 63, 'waste': 64, 'people': 65, 'real': 66, 'definitely': 67, 'case': 68, 'terrible': 69, 'money': 70, 'look': 71, 'new': 72, 'know': 73, 'experience': 74, 'came': 75, 'movies': 76, 'disappointed': 77, 'friendly': 78, 'many': 79, 'plot': 80, 'story': 81, 'far': 82, 'life': 83, 'poor': 84, 'want': 85, 'piece': 86, 'used': 87, 'worth': 88, 'long': 89, 'years': 90, 'nothing': 91, 'seen': 92, 'us': 93, 'lot': 94, 'quite': 95, 'wonderful': 96, 'restaurant': 97, 'happy': 98, 'screen': 99, 'give': 100, 'always': 101, 'films': 102, 'character': 103, 'script': 104, 'highly': 105, 'camera': 106, 'found': 107, 'anyone': 108, 'delicious': 109, 'easy': 110, 'times': 111, 'another': 112, 'watching': 113, 'vegas': 114, 'went': 115, 'bought': 116, 'absolutely': 117, 'car': 118, 'fine': 119, 'buy': 120, 'worked': 121, 'around': 122, 'funny': 123, 'probably': 124, 'however': 125, 'take': 126, 'awful': 127, 'eat': 128, 'comfortable': 129, 'loved': 130, 'since': 131, 'big': 132, 'item': 133, 'slow': 134, 'awesome': 135, 'thought': 136, 'horrible': 137, 'charger': 138, 'must': 139, 'makes': 140, 'end': 141, 'cool': 142, 'find': 143, 'music': 144, 'stars': 145, 'things': 146, 'show': 147, 'totally': 148, 'scenes': 149, 'actors': 150, 'watch': 151, 'staff': 152, 'impressed': 153, 'though': 154, 'last': 155, 'talk': 156, 'bluetooth': 157, 'black': 158, 'job': 159, 'small': 160, 'cheap': 161, 'fantastic': 162, 'night': 163, 'overall': 164, 'part': 165, 'family': 166, 'old': 167, 'day': 168, 'come': 169, 'cast': 170, 'line': 171, 'beautiful': 172, 'reception': 173, 'kind': 174, 'ordered': 175, 'stupid': 176, 'said': 177, 'feel': 178, 'perfect': 179, 'chicken': 180, 'problems': 181, 'problem': 182, 'sure': 183, 'performance': 184, 'actually': 185, 'done': 186, 'avoid': 187, 'try': 188, 'low': 189, 'especially': 190, 'next': 191, 'fresh': 192, 'special': 193, 'everyone': 194, 'charge': 195, 'least': 196, 'working': 197, 'order': 198, 'tried': 199, 'simply': 200, 'felt': 201, 'bit': 202, 'year': 203, 'customer': 204, 'anything': 205, 'sucks': 206, 'hard': 207, 'coming': 208, 'scene': 209, 'interesting': 210, 'menu': 211, 'salad': 212, 'pizza': 213, 'without': 214, 'hear': 215, 'calls': 216, 'looks': 217, 'purchase': 218, 'gets': 219, 'expect': 220, 'fit': 221, 'fast': 222, 'cell': 223, 'completely': 224, 'almost': 225, 'return': 226, 'either': 227, 'away': 228, 'man': 229, 'writing': 230, 'worse': 231, 'taste': 232, 'wait': 233, 'steak': 234, 'motorola': 235, 'left': 236, 'short': 237, 'clear': 238, 'disappoint': 239, 'seriously': 240, 'different': 241, 'device': 242, 'super': 243, 'using': 244, 'white': 245, 'call': 246, 'whole': 247, 'rather': 248, 'bland': 249, 'liked': 250, 'enjoy': 251, 'hour': 252, 'fact': 253, 'art': 254, 'enjoyed': 255, 'dialogue': 256, 'server': 257, 'sushi': 258, 'flavor': 259, 'plug': 260, 'volume': 261, 'several': 262, 'started': 263, 'design': 264, 'three': 265, 'play': 266, 'full': 267, 'may': 268, 'getting': 269, 'perfectly': 270, 'side': 271, 'put': 272, 'incredible': 273, 'extremely': 274, 'tell': 275, 'looking': 276, 'crap': 277, 'deal': 278, 'understand': 279, 'unfortunately': 280, 'took': 281, 'sucked': 282, 'believe': 283, 'saw': 284, 'buffet': 285, 'feeling': 286, 'director': 287, 'watched': 288, 'atmosphere': 289, 'burger': 290, 'yet': 291, 'need': 292, 'received': 293, 'huge': 294, 'high': 295, 'light': 296, 'fits': 297, 'hours': 298, 'less': 299, 'stay': 300, 'certainly': 301, 'mess': 302, 'disappointment': 303, 'hands': 304, 'care': 305, 'hot': 306, 'oh': 307, 'wrong': 308, 'star': 309, 'barely': 310, 'truly': 311, 'mediocre': 312, 'soon': 313, 'ending': 314, 'tasty': 315, 'meal': 316, 'fun': 317, 'wasted': 318, 'original': 319, 'days': 320, 'months': 321, 'kept': 322, 'picture': 323, 'phones': 324, 'world': 325, 'let': 326, 'series': 327, 'home': 328, 'strong': 329, 'set': 330, 'voice': 331, 'area': 332, 'glad': 333, 'company': 334, 'none': 335, 'something': 336, 'tv': 337, 'together': 338, 'effects': 339, 'easily': 340, 'inside': 341, 'top': 342, 'kids': 343, 'predictable': 344, 'cinematography': 345, 'actor': 346, 'played': 347, 'drama': 348, 'cold': 349, 'selection': 350, 'prices': 351, 'sauce': 352, 'lunch': 353, 'breakfast': 354, 'waited': 355, 'decent': 356, 'helpful': 357, 'turn': 358, 'priced': 359, 'verizon': 360, 'dropped': 361, 'seems': 362, 'wear': 363, 'broke': 364, 'free': 365, 'jabra': 366, 'wife': 367, 'pleased': 368, 'house': 369, 'check': 370, 'lost': 371, 'junk': 372, 'gave': 373, 'amazon': 374, 'ago': 375, 'couple': 376, 'face': 377, 'self': 378, 'trying': 379, 'today': 380, 'keep': 381, 'cannot': 382, 'guess': 383, 'bar': 384, 'else': 385, 'making': 386, 'places': 387, 'half': 388, 'directing': 389, 'game': 390, 'horror': 391, 'boring': 392, 'playing': 393, 'waitress': 394, 'second': 395, 'style': 396, 'spot': 397, 'fries': 398, 'table': 399, 'sandwich': 400, 'meat': 401, 'dishes': 402, 'clean': 403, 'value': 404, 'simple': 405, 'hate': 406, 'recommended': 407, 'garbage': 408, 'audio': 409, 'mind': 410, 'buttons': 411, 'perhaps': 412, 'arrived': 413, 'quickly': 414, 'pictures': 415, 'although': 416, 'ask': 417, 'obviously': 418, 'mostly': 419, 'expected': 420, 'important': 421, 'someone': 422, 'weak': 423, 'average': 424, 'finally': 425, 'waiting': 426, 'comes': 427, 'unit': 428, 'given': 429, 'quick': 430, 'plus': 431, 'point': 432, 'pay': 433, 'disappointing': 434, 'told': 435, 'entire': 436, 'fails': 437, 'word': 438, 'girl': 439, 'cinema': 440, 'amount': 441, 'twice': 442, 'suspense': 443, 'human': 444, 'beer': 445, 'dish': 446, 'dining': 447, 'spicy': 448, 'rude': 449, 'tasted': 450, 'unless': 451, 'book': 452, 'loud': 453, 'reasonable': 454, 'front': 455, 'cover': 456, 'week': 457, 'later': 458, 'feels': 459, 'headsets': 460, 'within': 461, 'software': 462, 'brilliant': 463, 'literally': 464, 'superb': 465, 'internet': 466, 'charm': 467, 'useless': 468, 'color': 469, 'room': 470, 'authentic': 471, 'cable': 472, 'nokia': 473, 'outside': 474, 'others': 475, 'wanted': 476, 'mention': 477, 'might': 478, 'solid': 479, 'review': 480, 'store': 481, 'friends': 482, 'course': 483, 'needed': 484, 'wall': 485, 'whatever': 486, 'management': 487, 'joy': 488, 'lines': 489, 'non': 490, 'hilarious': 491, 'budget': 492, 'single': 493, 'editing': 494, 'written': 495, 'annoying': 496, 'classic': 497, 'bring': 498, 'maybe': 499, 'possible': 500, 'beyond': 501, 'close': 502, 'john': 503, 'action': 504, 'hope': 505, 'location': 506, 'sick': 507, 'subtle': 508, 'seeing': 509, 'drive': 510, 'comedy': 511, 'heart': 512, 'tender': 513, 'served': 514, 'eating': 515, 'overpriced': 516, 'warm': 517, 'waiter': 518, 'attentive': 519, 'cooked': 520, 'town': 521, 'ambiance': 522, 'chips': 523, 'blue': 524, 'bars': 525, 'hold': 526, 'ears': 527, 'features': 528, 'instead': 529, 'large': 530, 'headphones': 531, 'particular': 532, 'player': 533, 'songs': 534, 'sometimes': 535, 'longer': 536, 'costs': 537, 'dont': 538, 'signal': 539, 'basically': 540, 'seemed': 541, 'goes': 542, 'whether': 543, 'hit': 544, 'thin': 545, 'bother': 546, 'range': 547, 'living': 548, 'child': 549, 'mistake': 550, 'whatsoever': 551, 'feature': 552, 'adorable': 553, 'poorly': 554, 'clever': 555, 'lacking': 556, 'reason': 557, 'exactly': 558, 'dead': 559, 'plastic': 560, 'note': 561, 'difficult': 562, 'seem': 563, 'zero': 564, 'please': 565, 'joke': 566, 'walked': 567, 'idea': 568, 'samsung': 569, 'cases': 570, 'extra': 571, 'reviews': 572, 'happened': 573, 'despite': 574, 'etc': 575, 'elsewhere': 576, 'hand': 577, 'rare': 578, 'lots': 579, 'rest': 580, 'wow': 581, 'ridiculous': 582, 'running': 583, 'lacks': 584, 'casting': 585, 'history': 586, 'flick': 587, 'lovely': 588, 'performances': 589, 'believable': 590, 'portrayal': 591, 'sat': 592, 'gives': 593, 'business': 594, 'terrific': 595, 'rent': 596, 'fish': 597, 'hitchcock': 598, 'holes': 599, 'considering': 600, 'memorable': 601, 'sweet': 602, 'become': 603, 'leave': 604, 'throughout': 605, 'looked': 606, 'damn': 607, 'pho': 608, 'shrimp': 609, 'potato': 610, 'dinner': 611, 'razr': 612, 'owner': 613, 'charging': 614, 'owned': 615, 'mobile': 616, 'provided': 617, 'moving': 618, 'sturdy': 619, 'party': 620, 'person': 621, 'choice': 622, 'holds': 623, 'bargain': 624, 'shipping': 625, 'impressive': 626, 'fall': 627, 'leather': 628, 'weeks': 629, 'stuff': 630, 'minute': 631, 'forever': 632, 'graphics': 633, 'offers': 634, 'options': 635, 'needs': 636, 'recently': 637, 'incredibly': 638, 'reasonably': 639, 'form': 640, 'embarrassing': 641, 'favorite': 642, 'cute': 643, 'buying': 644, 'cant': 645, 'games': 646, 'belt': 647, 'data': 648, 'along': 649, 'sides': 650, 'satisfied': 651, 'clarity': 652, 'wish': 653, 'turned': 654, 'reading': 655, 'computer': 656, 'ability': 657, 'roles': 658, 'ended': 659, 'replace': 660, 'touch': 661, 'total': 662, 'due': 663, 'trash': 664, 'connection': 665, 'wind': 666, 'rate': 667, 'beat': 668, 'shots': 669, 'generally': 670, 'break': 671, 'dirty': 672, 'utterly': 673, 'setting': 674, 'ok': 675, 'rating': 676, 'flat': 677, 'audience': 678, 'empty': 679, 'consider': 680, 'production': 681, 'level': 682, 'deserves': 683, 'lame': 684, 'tom': 685, 'often': 686, 'mouth': 687, 'list': 688, 'trip': 689, 'direction': 690, 'imagination': 691, 'cult': 692, 'bore': 693, 'mean': 694, 'lead': 695, 'silent': 696, 'thriller': 697, 'usual': 698, 'pathetic': 699, 'lacked': 700, 'plain': 701, 'soundtrack': 702, 'fans': 703, 'eyes': 704, 'water': 705, 'shot': 706, 'dry': 707, 'fan': 708, 'involved': 709, 'period': 710, 'particularly': 711, 'insult': 712, 'recent': 713, 'par': 714, 'unbelievable': 715, 'rice': 716, 'sub': 717, 'sad': 718, 'asked': 719, 'tables': 720, 'portions': 721, 'dessert': 722, 'beef': 723, 'servers': 724, 'seafood': 725, 'pasta': 726, 'tasteless': 727, 'thai': 728, 'eaten': 729, 'seated': 730, 'bacon': 731, 'ice': 732, 'bread': 733, 'friend': 734, 'fried': 735, 'anytime': 736, 'mic': 737, 'sending': 738, 'clip': 739, 'advise': 740, 'website': 741, 'pair': 742, 'run': 743, 'instructions': 744, 'pull': 745, 'included': 746, 'worthless': 747, 'thats': 748, 'keyboard': 749, 'turns': 750, 'e': 751, 'forget': 752, 'support': 753, 'clearly': 754, 'fairly': 755, 'trouble': 756, 'station': 757, 'purchased': 758, 'loves': 759, 'decision': 760, 'rated': 761, 'rocks': 762, 'number': 763, 'comfortably': 764, 'bt': 765, 'failed': 766, 'takes': 767, 'previous': 768, 'tool': 769, 'share': 770, 'wasting': 771, 'network': 772, 'likes': 773, 'lasts': 774, 'nearly': 775, 'anyway': 776, 'able': 777, 'issues': 778, 'lightweight': 779, 'hair': 780, 'market': 781, 'type': 782, 'earpiece': 783, 'unreliable': 784, 'seller': 785, 'plantronics': 786, 'charged': 787, 'strip': 788, 'lg': 789, 'gotten': 790, 'immediately': 791, 'easier': 792, 'appealing': 793, 'called': 794, 'happier': 795, 'starts': 796, 'effective': 797, 'weird': 798, 'appearance': 799, 'q': 800, 'expensive': 801, 'results': 802, 'im': 803, 'earlier': 804, 'palm': 805, 'missed': 806, 'cingular': 807, 'ringtones': 808, 'stop': 809, 'plays': 810, 'killer': 811, 'treo': 812, 'usb': 813, 'genuine': 814, 'pack': 815, 'red': 816, 'third': 817, 'ready': 818, 'chinese': 819, 'crisp': 820, 'video': 821, 'power': 822, 'passed': 823, 'shows': 824, 'cut': 825, 'somewhat': 826, 'cost': 827, 'son': 828, 'addition': 829, 'disgusting': 830, 'five': 831, 'yes': 832, 'thumbs': 833, 'checked': 834, 'exceptional': 835, 'wonderfully': 836, 'christmas': 837, 'directed': 838, 'young': 839, 'song': 840, 'head': 841, 'showed': 842, 'main': 843, 'tale': 844, 'greatest': 845, 'gem': 846, 'true': 847, 'words': 848, 'th': 849, 'massive': 850, 'pg': 851, 'complete': 852, 'green': 853, 'convincing': 854, 'honestly': 855, 'follow': 856, 'energy': 857, 'pretentious': 858, 'occasionally': 859, 'including': 860, 'chemistry': 861, 'fear': 862, 'depth': 863, 'bunch': 864, 'paid': 865, 'crazy': 866, 'storyline': 867, 'boyfriend': 868, 'gone': 869, 'already': 870, 'attempt': 871, 'torture': 872, 'pleasant': 873, 'premise': 874, 'ups': 875, 'age': 876, 'ray': 877, 'charles': 878, 'remember': 879, 'attention': 880, 'master': 881, 'visual': 882, 'mexican': 883, 'indeed': 884, 'thoroughly': 885, 'theater': 886, 'intelligence': 887, 'intelligent': 888, 'sense': 889, 'entertaining': 890, 'bored': 891, 'guy': 892, 'towards': 893, 'drago': 894, 'enjoyable': 895, 'god': 896, 'parts': 897, 'sets': 898, 'stories': 899, 'created': 900, 'role': 901, 'scamp': 902, 'charming': 903, 'death': 904, 'rolls': 905, 'space': 906, 'conclusion': 907, 'ten': 908, 'final': 909, 'serious': 910, 'mickey': 911, 'crowd': 912, 'james': 913, 'vibe': 914, 'dark': 915, 'honest': 916, 'known': 917, 'nasty': 918, 'delight': 919, 'thinking': 920, 'heard': 921, 'dance': 922, 'italian': 923, 'beginning': 924, 'appreciate': 925, 'tacos': 926, 'salmon': 927, 'visit': 928, 'establishment': 929, 'husband': 930, 'drinks': 931, 'yummy': 932, 'wine': 933, 'steaks': 934, 'pork': 935, 'burgers': 936, 'cream': 937, 'drink': 938, 'bay': 939, 'phoenix': 940, 'wings': 941, 'live': 942, 'folks': 943, 'chef': 944, 'tea': 945, 'vegetables': 946, 'soup': 947, 'owners': 948, 'manager': 949, 'jawbone': 950, 'conversations': 951, 'lasting': 952, 'contacts': 953, 'imagine': 954, 'needless': 955, 'static': 956, 'odd': 957, 'wonder': 958, 'fire': 959, 'mother': 960, 'pocket': 961, 'pc': 962, 'combination': 963, 'speed': 964, 'mp': 965, 'gonna': 966, 'returned': 967, 'protection': 968, 'machine': 969, 'seconds': 970, 'complaints': 971, 'glasses': 972, 'quiet': 973, 'saying': 974, 'packaged': 975, 'construction': 976, 'boy': 977, 'loads': 978, 'ease': 979, 'plan': 980, 'beware': 981, 'supposedly': 982, 'match': 983, 'pros': 984, 'flaw': 985, 'expectations': 986, 'display': 987, 'sex': 988, 'setup': 989, 'earpieces': 990, 'earbud': 991, 'coverage': 992, 'tremendous': 993, 'experienced': 994, 'drops': 995, 'description': 996, 'sounds': 997, 'wi': 998, 'sharp': 999, 'chargers': 1000, 'handsfree': 1001, 'v': 1002, 'bucks': 1003, 'replacement': 1004, 'strange': 1005, 'means': 1006, 'sony': 1007, 'comfort': 1008, 'aspect': 1009, 'appears': 1010, 'scary': 1011, 'drain': 1012, 'fry': 1013, 'giving': 1014, 'scratched': 1015, 'wise': 1016, 'microphone': 1017, 'uncomfortable': 1018, 'plugged': 1019, 'driving': 1020, 'neither': 1021, 'flip': 1022, 'inexpensive': 1023, 'sitting': 1024, 'los': 1025, 'angeles': 1026, 'wireless': 1027, 'save': 1028, 'help': 1029, 'era': 1030, 'breaks': 1031, 'effect': 1032, 'warning': 1033, 'alone': 1034, 'size': 1035, 'fi': 1036, 'wearing': 1037, 'sunglasses': 1038, 'changing': 1039, 'worthwhile': 1040, 'pricing': 1041, 'regret': 1042, 'user': 1043, 'pitiful': 1044, 'noise': 1045, 'holster': 1046, 'eye': 1047, 'amazed': 1048, 'orders': 1049, 'thanks': 1050, 'edge': 1051, 'refund': 1052, 'drop': 1053, 'ones': 1054, 'properly': 1055, 'flawed': 1056, 'hated': 1057, 'forced': 1058, 'speaker': 1059, 'constructed': 1060, 'holding': 1061, 'sprint': 1062, 'effort': 1063, 'breaking': 1064, 'stopped': 1065, 'apart': 1066, 'surprised': 1067, 'fabulous': 1068, 'tinny': 1069, 'overly': 1070, 'placed': 1071, 'keeps': 1072, 'falling': 1073, 'tiny': 1074, 'four': 1075, 'spring': 1076, 'tries': 1077, 'interested': 1078, 'continue': 1079, 'pairing': 1080, 'balance': 1081, 'explain': 1082, 'biggest': 1083, 'stand': 1084, 'occupied': 1085, 'ripped': 1086, 'mini': 1087, 'r': 1088, 'iphone': 1089, 'multiple': 1090, 'outlet': 1091, 'freedom': 1092, 'mark': 1093, 'plenty': 1094, 'convenient': 1095, 'date': 1096, 'bottom': 1097, 'wooden': 1098, 'boot': 1099, 'sorry': 1100, 'double': 1101, 'sounded': 1102, 'feet': 1103, 'everywhere': 1104, 'father': 1105, 'send': 1106, 'describe': 1107, 'letting': 1108, 'material': 1109, 'flaws': 1110, 'beep': 1111, 'says': 1112, 'managed': 1113, 'seat': 1114, 'otherwise': 1115, 'control': 1116, 'complain': 1117, 'lousy': 1118, 'painful': 1119, 'speak': 1120, 'meaning': 1121, 'baby': 1122, 'delivers': 1123, 'sea': 1124, 'faux': 1125, 'significant': 1126, 'pure': 1127, 'brilliance': 1128, 'moment': 1129, 'talented': 1130, 'fare': 1131, 'hill': 1132, 'ed': 1133, 'ladies': 1134, 'grace': 1135, 'negative': 1136, 'cause': 1137, 'pointless': 1138, 'anne': 1139, 'children': 1140, 'parents': 1141, 'dialog': 1142, 'selections': 1143, 'gross': 1144, 'provoking': 1145, 'paced': 1146, 'lion': 1147, 'acted': 1148, 'checking': 1149, 'touching': 1150, 'scenery': 1151, 'paper': 1152, 'mature': 1153, 'episode': 1154, 'remake': 1155, 'nobody': 1156, 'putting': 1157, 'conflict': 1158, 'handled': 1159, 'hell': 1160, 'disaster': 1161, 'treat': 1162, 'yeah': 1163, 'girlfriend': 1164, 'stereotypes': 1165, 'cartoon': 1166, 'paul': 1167, 'women': 1168, 'brain': 1169, 'redeeming': 1170, 'appalling': 1171, 'presents': 1172, 'screenwriter': 1173, 'step': 1174, 'industry': 1175, 'indulgent': 1176, 'spent': 1177, 'daughter': 1178, 'singing': 1179, 'dancing': 1180, 'dvd': 1181, 'struck': 1182, 'contained': 1183, 'theme': 1184, 'aerial': 1185, 'thrilled': 1186, 'deeply': 1187, 'interest': 1188, 'narrative': 1189, 'actress': 1190, 'spoilers': 1191, 'stunning': 1192, 'fx': 1193, 'surprisingly': 1194, 'released': 1195, 'ranks': 1196, 'journey': 1197, 'memories': 1198, 'summary': 1199, 'billy': 1200, 'possibly': 1201, 'trilogy': 1202, 'favourite': 1203, 'judge': 1204, 'wrap': 1205, 'leaves': 1206, 'reviewer': 1207, 'smart': 1208, 'bakery': 1209, 'stage': 1210, 'actresses': 1211, 'ways': 1212, 'modern': 1213, 'scared': 1214, 'supposed': 1215, 'learn': 1216, 'values': 1217, 'photography': 1218, 'huston': 1219, 'steve': 1220, 'ruthless': 1221, 'war': 1222, 'example': 1223, 'angel': 1224, 'clichs': 1225, 'thrown': 1226, 'nut': 1227, 'scale': 1228, 'glance': 1229, 'score': 1230, 'frightening': 1231, 'oscar': 1232, 'knew': 1233, 'stanwyck': 1234, 'fat': 1235, 'footage': 1236, 'opened': 1237, 'sort': 1238, 'behind': 1239, 'race': 1240, 'happen': 1241, 'unconvincing': 1242, 'produced': 1243, 'early': 1244, 'documentary': 1245, 'martin': 1246, 'racial': 1247, 'imaginative': 1248, 'credit': 1249, 'watchable': 1250, 'pace': 1251, 'lighting': 1252, 'past': 1253, 'excuse': 1254, 'nonsense': 1255, 'exquisite': 1256, 'south': 1257, 'lady': 1258, 'delightful': 1259, 'beautifully': 1260, 'wilkinson': 1261, 'laughable': 1262, 'begin': 1263, 'late': 1264, 'giallo': 1265, 'emotions': 1266, 'belly': 1267, 'spend': 1268, 'write': 1269, 'stinks': 1270, 'writer': 1271, 'pm': 1272, 'suck': 1273, 'stayed': 1274, 'puppets': 1275, 'doubt': 1276, 'guys': 1277, 'hip': 1278, 'miyazaki': 1279, 'realized': 1280, 'animation': 1281, 'added': 1282, 'texture': 1283, 'recommendation': 1284, 'potatoes': 1285, 'brought': 1286, 'sashimi': 1287, 'moist': 1288, 'frozen': 1289, 'greek': 1290, 'dressing': 1291, 'pita': 1292, 'hummus': 1293, 'duck': 1294, 'salt': 1295, 'chewy': 1296, 'min': 1297, 'tip': 1298, 'cafe': 1299, 'ambience': 1300, 'busy': 1301, 'delish': 1302, 'melt': 1303, 'cheese': 1304, 'subway': 1305, 'restaurants': 1306, 'ate': 1307, 'stale': 1308, 'treated': 1309, 'vegetarian': 1310, 'healthy': 1311, 'decor': 1312, 'butter': 1313, 'egg': 1314, 'dog': 1315, 'brunch': 1316, 'soggy': 1317, 'lobster': 1318, 'flavorful': 1319, 'generous': 1320, 'patio': 1321, 'outstanding': 1322, 'pizzas': 1323, 'pulled': 1324, 'mom': 1325, 'equally': 1326, 'bathroom': 1327, 'door': 1328, 'stomach': 1329, 'beans': 1330, 'regular': 1331, 'bill': 1332, 'meals': 1333, 'homemade': 1334, 'desserts': 1335, 'edible': 1336, 'seating': 1337, 'salsa': 1338, 'major': 1339, 'dozen': 1340, 'extended': 1341, 'notice': 1342, 'tooth': 1343, 'unusable': 1344, 'contract': 1345, 'juice': 1346, 'regarding': 1347, 'pda': 1348, 'neat': 1349, 'gadget': 1350, 'essentially': 1351, 'microsoft': 1352, 'tech': 1353, 'elegant': 1354, 'angle': 1355, 'skip': 1356, 'lock': 1357, 'died': 1358, 'situations': 1359, 'docking': 1360, 'advertised': 1361, 'handy': 1362, 'cheaper': 1363, 'seamlessly': 1364, 'buyer': 1365, 'apparently': 1366, 'relatively': 1367, 'resolution': 1368, 'slim': 1369, 'toast': 1370, 'sleek': 1371, 'compromise': 1372, 'basic': 1373, 'keypad': 1374, 'unhappy': 1375, 'winner': 1376, 'realize': 1377, 'careful': 1378, 'logitech': 1379, 'recognition': 1380, 'stated': 1381, 'blackberry': 1382, 'technology': 1383, 'wired': 1384, 'messages': 1385, 'faster': 1386, 'build': 1387, 'whine': 1388, 'maintain': 1389, 'humans': 1390, 'button': 1391, 'thank': 1392, 'igo': 1393, 'tips': 1394, 'connected': 1395, 'latest': 1396, 'g': 1397, 'storage': 1398, 'buzzing': 1399, 'override': 1400, 'functionality': 1401, 'ring': 1402, 'tone': 1403, 'dropping': 1404, 'weight': 1405, 'hardly': 1406, 'ends': 1407, 'consumer': 1408, 'background': 1409, 'usually': 1410, 'excited': 1411, 'additional': 1412, 'gels': 1413, 'purpose': 1414, 'secure': 1415, 'gosh': 1416, 'attractive': 1417, 'rubber': 1418, 'smell': 1419, 'unbearable': 1420, 'caused': 1421, 'flimsy': 1422, 'month': 1423, 'flawlessly': 1424, 'rotating': 1425, 'thru': 1426, 'cat': 1427, 'vi': 1428, 'removing': 1429, 'compliments': 1430, 'state': 1431, 'allow': 1432, 'dialing': 1433, 'ipod': 1434, 'recharge': 1435, 'practically': 1436, 'add': 1437, 'boost': 1438, 'finished': 1439, 'ringing': 1440, 'auto': 1441, 'reverse': 1442, 'hurt': 1443, 'push': 1444, 'skype': 1445, 'shipped': 1446, 'promptly': 1447, 'prompt': 1448, 'noticed': 1449, 'colored': 1450, 'cheaply': 1451, 'att': 1452, 'distorted': 1453, 'forgot': 1454, 'model': 1455, 'finds': 1456, 'enter': 1457, 'modest': 1458, 'dying': 1459, 'built': 1460, 'install': 1461, 'purchasing': 1462, 'moto': 1463, 'figure': 1464, 'key': 1465, 'fully': 1466, 'memory': 1467, 'card': 1468, 'hat': 1469, 'timely': 1470, 'update': 1471, 'cumbersome': 1472, 'delivery': 1473, 'vx': 1474, 'switch': 1475, 'batteries': 1476, 'receiving': 1477, 'exchanged': 1478, 'cellphone': 1479, 'described': 1480, 'nyc': 1481, 'defective': 1482, 'unacceptable': 1483, 'catching': 1484, 'function': 1485, 'timeframe': 1486, 'complaint': 1487, 'standard': 1488, 'ugly': 1489, 'improvement': 1490, 'accidentally': 1491, 'listening': 1492, 'kitchen': 1493, 'conversation': 1494, 'practical': 1495, 'ample': 1496, 'eargels': 1497, 'channel': 1498, 'numerous': 1499, 'frustration': 1500, 'understanding': 1501, 'wire': 1502, 'menus': 1503, 'broken': 1504, 'development': 1505, 'knows': 1506, 'operate': 1507, 'paired': 1508, 'normally': 1509, 'brand': 1510, 'players': 1511, 'comments': 1512, 'echo': 1513, 'warranty': 1514, 'produce': 1515, 'luck': 1516, 'exchange': 1517, 'utter': 1518, 'download': 1519, 'designed': 1520, 'smoothly': 1521, 'study': 1522, 'access': 1523, 'somehow': 1524, 'flash': 1525, 'choices': 1526, 'tones': 1527, 'prime': 1528, 'jack': 1529, 'accept': 1530, 'except': 1531, 'allows': 1532, 'open': 1533, 'allowing': 1534, 'numbers': 1535, 'crashed': 1536, 'quit': 1537, 'via': 1538, 'span': 1539, 'signs': 1540, 'sizes': 1541, 'provides': 1542, 'classy': 1543, 'mainly': 1544, 'metro': 1545, 'smoke': 1546, 'carries': 1547, 'highest': 1548, 'protector': 1549, 'wont': 1550, 'hoping': 1551, 'impossible': 1552, 'refused': 1553, 'discount': 1554, 'ps': 1555, 'talking': 1556, 'telephone': 1557, 'ordering': 1558, 'awkward': 1559, 'row': 1560, 'current': 1561, 'answer': 1562, 'read': 1563, 'laptop': 1564, 'sister': 1565, 'discomfort': 1566, 'normal': 1567, 'confusing': 1568, 'lose': 1569, 'cutouts': 1570, 'cuts': 1571, 'wild': 1572, 'carry': 1573, 'loose': 1574, 'receive': 1575, 'lightly': 1576, 'certain': 1577, 'unsatisfactory': 1578, 'hype': 1579, 'covered': 1580, 'falls': 1581, 'touches': 1582, 'angles': 1583, 'became': 1584, 'jimmy': 1585, 'sisters': 1586, 'terms': 1587, 'masterpiece': 1588, 'masterpieces': 1589, 'fill': 1590, 'imaginable': 1591, 'pieces': 1592, 'create': 1593, 'levels': 1594, 'canada': 1595, 'sequel': 1596, 'unfunny': 1597, 'generic': 1598, 'morgan': 1599, 'jonah': 1600, 'lazy': 1601, 'presence': 1602, 'obvious': 1603, 'lesser': 1604, 'french': 1605, 'whiny': 1606, 'future': 1607, 'warmth': 1608, 'delivering': 1609, 'unpredictable': 1610, 'badly': 1611, 'alexander': 1612, 'artist': 1613, 'lived': 1614, 'superbly': 1615, 'b': 1616, 'heaven': 1617, 'lives': 1618, 'church': 1619, 'tonight': 1620, 'uplifting': 1621, 'frankly': 1622, 'club': 1623, 'lane': 1624, 'chick': 1625, 'correct': 1626, 'bold': 1627, 'helps': 1628, 'idiot': 1629, 'accused': 1630, 'afraid': 1631, 'tremendously': 1632, 'sandra': 1633, 'bullock': 1634, 'moments': 1635, 'losing': 1636, 'ratings': 1637, 'dramatic': 1638, 'tension': 1639, 'central': 1640, 'themes': 1641, 'core': 1642, 'following': 1643, 'viewing': 1644, 'quinn': 1645, 'disliked': 1646, 'dollars': 1647, 'mad': 1648, 'cardboard': 1649, 'predictably': 1650, 'crafted': 1651, 'depressing': 1652, 'racism': 1653, 'charisma': 1654, 'explanation': 1655, 'drag': 1656, 'wayne': 1657, 'older': 1658, 'noteworthy': 1659, 'blood': 1660, 'forth': 1661, 'jamie': 1662, 'genius': 1663, 'hence': 1664, 'mishima': 1665, 'uninteresting': 1666, 'chilly': 1667, 'culture': 1668, 'schrader': 1669, 'realistic': 1670, 'talent': 1671, 'chance': 1672, 'senses': 1673, 'june': 1674, 'angry': 1675, 'sour': 1676, 'matter': 1677, 'noir': 1678, 'complex': 1679, 'psychological': 1680, 'soul': 1681, 'gripping': 1682, 'disturbing': 1683, 'jerky': 1684, 'camerawork': 1685, 'witty': 1686, 'ceases': 1687, 'directors': 1688, 'visually': 1689, 'spoiler': 1690, 'remaining': 1691, 'suffering': 1692, 'smile': 1693, 'vomited': 1694, 'coppola': 1695, 'unfolds': 1696, 'contrast': 1697, 'sublime': 1698, 'offensive': 1699, 'poetry': 1700, 'masterful': 1701, 'nature': 1702, 'disbelief': 1703, 'female': 1704, 'array': 1705, 'nuts': 1706, 'dangerous': 1707, 'reactions': 1708, 'twist': 1709, 'shed': 1710, 'underneath': 1711, 'wide': 1712, 'relationship': 1713, 'assistant': 1714, 'laugh': 1715, 'de': 1716, 'taped': 1717, 'wholesome': 1718, 'portraying': 1719, 'taking': 1720, 'forgetting': 1721, 'attempts': 1722, 'situation': 1723, 'bear': 1724, 'kinda': 1725, 'question': 1726, 'lucy': 1727, 'japanese': 1728, 'america': 1729, 'composition': 1730, 'brian': 1731, 'largely': 1732, 'member': 1733, 'identify': 1734, 'connery': 1735, 'robert': 1736, 'murdered': 1737, 'humour': 1738, 'brother': 1739, 'ryan': 1740, 'hollywood': 1741, 'thus': 1742, 'undoubtedly': 1743, 'popular': 1744, 'positive': 1745, 'unique': 1746, 'european': 1747, 'circumstances': 1748, 'blow': 1749, 'caught': 1750, 'junkyard': 1751, 'dogs': 1752, 'american': 1753, 'americans': 1754, 'pile': 1755, 'round': 1756, 'continuity': 1757, 'corn': 1758, 'box': 1759, 'directorial': 1760, 'proud': 1761, 'sand': 1762, 'sake': 1763, 'jobs': 1764, 'public': 1765, 'fox': 1766, 'unrecognizable': 1767, 'relations': 1768, 'taylor': 1769, 'surprising': 1770, 'entirely': 1771, 'chow': 1772, 'cheesy': 1773, 'proceedings': 1774, 'remotely': 1775, 'mercy': 1776, 'killing': 1777, 'overacting': 1778, 'awkwardly': 1779, 'english': 1780, 'hackneyed': 1781, 'blah': 1782, 'balanced': 1783, 'length': 1784, 'air': 1785, 'afternoon': 1786, 'creates': 1787, 'fascinating': 1788, 'humorous': 1789, 'filmed': 1790, 'thoughts': 1791, 'dislike': 1792, 'began': 1793, 'change': 1794, 'season': 1795, 'contains': 1796, 'reality': 1797, 'century': 1798, 'implausible': 1799, 'jean': 1800, 'studio': 1801, 'indoor': 1802, 'start': 1803, 'middle': 1804, 'class': 1805, 'betty': 1806, 'aside': 1807, 'mouse': 1808, 'famous': 1809, 'ground': 1810, 'pleaser': 1811, 'turkey': 1812, 'among': 1813, 'terribly': 1814, 'deserving': 1815, 'shame': 1816, 'version': 1817, 'dumb': 1818, 'zombie': 1819, 'monster': 1820, 'lack': 1821, 'notable': 1822, 'opening': 1823, 'sequence': 1824, 'gas': 1825, 'fair': 1826, 'teeth': 1827, 'sum': 1828, 'avoided': 1829, 'limited': 1830, 'sign': 1831, 'asleep': 1832, 'cartoons': 1833, 'deserved': 1834, 'element': 1835, 'sit': 1836, 'fifteen': 1837, 'volcano': 1838, 'abysmal': 1839, 'vomit': 1840, 'rat': 1841, 'miss': 1842, 'dedication': 1843, 'versus': 1844, 'brings': 1845, 'focus': 1846, 'underlying': 1847, 'relationships': 1848, 'engaging': 1849, 'errol': 1850, 'flynn': 1851, 'dull': 1852, 'guests': 1853, 'surrounding': 1854, 'excellently': 1855, 'tops': 1856, 'typical': 1857, 'sci': 1858, 'revealing': 1859, 'africa': 1860, 'process': 1861, 'shakespear': 1862, 'macbeth': 1863, 'brief': 1864, 'moral': 1865, 'decay': 1866, 'forces': 1867, 'evil': 1868, 'adaptation': 1869, 'costumes': 1870, 'ball': 1871, 'judging': 1872, 'difference': 1873, 'documentaries': 1874, 'joe': 1875, 'watkins': 1876, 'peter': 1877, 'hollow': 1878, 'expert': 1879, 'ass': 1880, 'cole': 1881, 'disgrace': 1882, 'sleep': 1883, 'achievement': 1884, 'kid': 1885, 'tough': 1886, 'whenever': 1887, 'earth': 1888, 'emily': 1889, 'watson': 1890, 'prepared': 1891, 'compelling': 1892, 'sentiment': 1893, 'trap': 1894, 'fulci': 1895, 'genre': 1896, 'personally': 1897, 'thick': 1898, 'blown': 1899, 'events': 1900, 'result': 1901, 'slightest': 1902, 'duet': 1903, 'astronaut': 1904, 'doctor': 1905, 'extraordinary': 1906, 'courtroom': 1907, 'highlights': 1908, 'discovering': 1909, 'court': 1910, 'understated': 1911, 'based': 1912, 'smells': 1913, 'male': 1914, 'phenomenal': 1915, 'opinion': 1916, 'neil': 1917, 'delights': 1918, 'sample': 1919, 'deliver': 1920, 'sharply': 1921, 'tongue': 1922, 'cheek': 1923, 'struggle': 1924, 'heads': 1925, 'reminded': 1926, 'childhood': 1927, 'lord': 1928, 'atrocious': 1929, 'spoil': 1930, 'references': 1931, 'act': 1932, 'pacing': 1933, 'consistent': 1934, 'dreams': 1935, 'uses': 1936, 'hopefully': 1937, 'loneliness': 1938, 'emilio': 1939, 'estevez': 1940, 'interplay': 1941, 'suggest': 1942, 'natural': 1943, 'street': 1944, 'readers': 1945, 'hearts': 1946, 'closed': 1947, 'depicts': 1948, 'reasons': 1949, 'ponyo': 1950, 'refreshing': 1951, 'traditional': 1952, 'wants': 1953, 'wave': 1954, 'deep': 1955, 'lange': 1956, 'crust': 1957, 'cashier': 1958, 'mmmm': 1959, 'overwhelmed': 1960, 'bite': 1961, 'familiar': 1962, 'favor': 1963, 'grossed': 1964, 'char': 1965, 'attitudes': 1966, 'customers': 1967, 'portion': 1968, 'attack': 1969, 'grill': 1970, 'downtown': 1971, 'scallop': 1972, 'refill': 1973, 'appetizers': 1974, 'batter': 1975, 'finish': 1976, 'meh': 1977, 'seasoned': 1978, 'opportunity': 1979, 'underwhelming': 1980, 'grease': 1981, 'roast': 1982, 'sugary': 1983, 'six': 1984, 'die': 1985, 'bye': 1986, 'serves': 1987, 'roasted': 1988, 'garlic': 1989, 'marrow': 1990, 'mary': 1991, 'bartender': 1992, 'lovers': 1993, 'preparing': 1994, 'indian': 1995, 'crispy': 1996, 'tuna': 1997, 'bagels': 1998, 'dine': 1999, 'rarely': 2000, 'curry': 2001, 'bathrooms': 2002, 'decorated': 2003, 'greeted': 2004, 'joint': 2005, 'overcooked': 2006, 'charcoal': 2007, 'decided': 2008, 'dirt': 2009, 'gyros': 2010, 'valley': 2011, 'bowl': 2012, 'insulted': 2013, 'disrespected': 2014, 'stepped': 2015, 'gold': 2016, 'puree': 2017, 'bug': 2018, 'shower': 2019, 'bisque': 2020, 'filet': 2021, 'pepper': 2022, 'cook': 2023, 'dealing': 2024, 'cheeseburger': 2025, 'yum': 2026, 'mayo': 2027, 'building': 2028, 'creamy': 2029, 'similar': 2030, 'sticks': 2031, 'tap': 2032, 'coffee': 2033, 'boba': 2034, 'taco': 2035, 'bachi': 2036, 'salads': 2037, 'neighborhood': 2038, 'soooo': 2039, 'stir': 2040, 'summer': 2041, 'toasted': 2042, 'serve': 2043, 'bites': 2044, 'omg': 2045, 'brick': 2046, 'oven': 2047, 'pancakes': 2048, 'eggs': 2049, 'evening': 2050, 'lukewarm': 2051, 'eggplant': 2052, 'stuffed': 2053, 'mall': 2054, 'perfection': 2055, 'impeccable': 2056, 'pop': 2057, 'assure': 2058, 'professional': 2059, 'nicest': 2060, 'biscuits': 2061, 'cow': 2062, 'driest': 2063, 'tots': 2064, 'acknowledged': 2065, 'margaritas': 2066, 'flower': 2067, 'group': 2068, 'crab': 2069, 'legs': 2070, 'sliced': 2071, 'filling': 2072, 'choose': 2073, 'entrees': 2074, 'tapas': 2075, 'vinegrette': 2076, 'helped': 2077, 'presentation': 2078, 'satisfying': 2079, 'grilled': 2080, 'focused': 2081, 'promise': 2082, 'legit': 2083, 'staying': 2084, 'fail': 2085, 'plate': 2086, 'serving': 2087, 'fly': 2088, 'mid': 2089, 'definately': 2090, 'flavorless': 2091, 'nachos': 2092, 'tribute': 2093, 'fell': 2094, 'services': 2095, 'heat': 2096, 'undercooked': 2097, 'converter': 2098, 'tied': 2099, 'jiggle': 2100, 'hundred': 2101, 'seperated': 2102, 'mere': 2103, 'ft': 2104, 'excessive': 2105, 'garbled': 2106, 'fooled': 2107, 'clicks': 2108, 'mechanism': 2109, 'followed': 2110, 'directions': 2111, 'kindle': 2112, 'commercials': 2113, 'misleading': 2114, 'couldnt': 2115, 'earphone': 2116, 'breakage': 2117, 'unacceptible': 2118, 'ideal': 2119, 'whose': 2120, 'sensitive': 2121, 'freeway': 2122, 'ac': 2123, 'highy': 2124, 'mins': 2125, 'pics': 2126, 'arguing': 2127, 'bulky': 2128, 'usable': 2129, 'useful': 2130, 'stream': 2131, 'submerged': 2132, 'faceplates': 2133, 'drawback': 2134, 'pause': 2135, 'activated': 2136, 'suddenly': 2137, 'ipods': 2138, 'bmw': 2139, 'hearing': 2140, 'wrongly': 2141, 'everyday': 2142, 'intended': 2143, 'runs': 2144, 'greater': 2145, 'buds': 2146, 'waaay': 2147, 'bluetooths': 2148, 'listener': 2149, 'integrated': 2150, 'flush': 2151, 'toilet': 2152, 'styles': 2153, 'correctly': 2154, 'megapixels': 2155, 'renders': 2156, 'images': 2157, 'purcashed': 2158, 'geeky': 2159, 'oozes': 2160, 'embedded': 2161, 'stylish': 2162, 'qwerty': 2163, 'simpler': 2164, 'iam': 2165, 'disapoinment': 2166, 'accompanied': 2167, 'nicely': 2168, 'damage': 2169, 'definitly': 2170, 'majority': 2171, 'peachy': 2172, 'keen': 2173, 'upstairs': 2174, 'basement': 2175, 'reccomendation': 2176, 'relative': 2177, 'items': 2178, 'sudden': 2179, 'hop': 2180, 'linking': 2181, 'curve': 2182, 'sketchy': 2183, 'web': 2184, 'browsing': 2185, 'significantly': 2186, 'unlike': 2187, 'colors': 2188, 'communications': 2189, 'communicate': 2190, 'monkeys': 2191, 'dna': 2192, 'copy': 2193, 'bougth': 2194, 'lc': 2195, 'mode': 2196, 'file': 2197, 'browser': 2198, 'hs': 2199, 'os': 2200, 'crawl': 2201, 'recognizes': 2202, 'bluetoooth': 2203, 'thorn': 2204, 'abhor': 2205, 'disconnected': 2206, 'mail': 2207, 'backlight': 2208, 'message': 2209, 'lately': 2210, 'wit': 2211, 'impress': 2212, 'pleather': 2213, 'deaf': 2214, 'prettier': 2215, 'investment': 2216, 'ticking': 2217, 'noises': 2218, 'electronics': 2219, 'available': 2220, 'fm': 2221, 'transmitters': 2222, 'h': 2223, 'mega': 2224, 'pixel': 2225, 'transmit': 2226, 'contacting': 2227, 'dollar': 2228, 'learned': 2229, 'lesson': 2230, 'online': 2231, 'earbugs': 2232, 'roam': 2233, 'crack': 2234, 'infatuated': 2235, 'freezes': 2236, 'frequently': 2237, 'tick': 2238, 'headbands': 2239, 'ericsson': 2240, 'purchases': 2241, 'shine': 2242, 'calendar': 2243, 'sync': 2244, 'defeats': 2245, 'penny': 2246, 'wallet': 2247, 'excrutiatingly': 2248, 'glove': 2249, 'durable': 2250, 'factor': 2251, 'petroleum': 2252, 'stereo': 2253, 'absolutel': 2254, 'potentially': 2255, 'reversible': 2256, 'contstruct': 2257, 'hinge': 2258, 'installed': 2259, 'overnite': 2260, 'handset': 2261, 'attacked': 2262, 'protective': 2263, 'destroying': 2264, 'razor': 2265, 'shouldve': 2266, 'invented': 2267, 'sooner': 2268, 'engineered': 2269, 'complained': 2270, 'tracfone': 2271, 'instruction': 2272, 'manual': 2273, 'alarm': 2274, 'clock': 2275, 'antena': 2276, 'compared': 2277, 'usage': 2278, 'ngage': 2279, 'earbuds': 2280, 'riingtones': 2281, 'rip': 2282, 'frequentyly': 2283, 'adhesive': 2284, 'concrete': 2285, 'knock': 2286, 'wood': 2287, 'transformed': 2288, 'organizational': 2289, 'capability': 2290, 'vehicle': 2291, 'cradle': 2292, 'jerks': 2293, 'starter': 2294, 'loudspeaker': 2295, 'option': 2296, 'bumpers': 2297, 'lights': 2298, 'improve': 2299, 'leaks': 2300, 'according': 2301, 'applifies': 2302, 'specially': 2303, 'transmission': 2304, 'drivng': 2305, 'tape': 2306, 'embarassing': 2307, 'protects': 2308, 'operates': 2309, 'soyo': 2310, 'portraits': 2311, 'exterior': 2312, 'mentioned': 2313, 'gadgets': 2314, 'magical': 2315, 'comparably': 2316, 'offering': 2317, 'encourage': 2318, 'recieve': 2319, 'cradles': 2320, 'kits': 2321, 'excelent': 2322, 'cingulair': 2323, 'nicer': 2324, 'hoursthe': 2325, 'thereplacement': 2326, 'yell': 2327, 'iriver': 2328, 'spinn': 2329, 'fond': 2330, 'magnetic': 2331, 'strap': 2332, 'psyched': 2333, 'appointments': 2334, 'sanyo': 2335, 'survived': 2336, 'dozens': 2337, 'blacktop': 2338, 'ill': 2339, 'earphones': 2340, 'cellular': 2341, 'awsome': 2342, 'drained': 2343, 'earpad': 2344, 'displease': 2345, 'defect': 2346, 'risk': 2347, 'restored': 2348, 'jx': 2349, 'searched': 2350, 'pad': 2351, 'lit': 2352, 'portable': 2353, 'colleague': 2354, 'bed': 2355, 'morning': 2356, 'shipment': 2357, 'surefire': 2358, 'gx': 2359, 'buyers': 2360, 'remorse': 2361, 'accessoryone': 2362, 'inexcusable': 2363, 'returning': 2364, 'carriers': 2365, 'tmobile': 2366, 'procedure': 2367, 'motorolas': 2368, 'env': 2369, 'rocketed': 2370, 'destination': 2371, 'unknown': 2372, 'conditions': 2373, 'usefulness': 2374, 'bills': 2375, 'plans': 2376, 'overnight': 2377, 'respect': 2378, 'stuck': 2379, 'max': 2380, 'mute': 2381, 'hybrid': 2382, 'palmtop': 2383, 'excels': 2384, 'btv': 2385, 'bose': 2386, 'cancelling': 2387, 'commuter': 2388, 'photo': 2389, 'ad': 2390, 'noted': 2391, 'happens': 2392, 'frog': 2393, 'pushed': 2394, 'aluminum': 2395, 'protected': 2396, 'handheld': 2397, 'tools': 2398, 'sturdiness': 2399, 'source': 2400, 'waterproof': 2401, 'sliding': 2402, 'pants': 2403, 'pockets': 2404, 'shield': 2405, 'incrediable': 2406, 'refuse': 2407, 'activate': 2408, 'gentle': 2409, 'threw': 2410, 'window': 2411, 'inches': 2412, 'counter': 2413, 'cracked': 2414, 'laughing': 2415, 'trunk': 2416, 'carried': 2417, 'hitch': 2418, 'directly': 2419, 'increase': 2420, 'shifting': 2421, 'bubbling': 2422, 'peeling': 2423, 'scratch': 2424, 'droid': 2425, 'exercise': 2426, 'earset': 2427, 'outgoing': 2428, 'package': 2429, 'patient': 2430, 'wirefly': 2431, 'contact': 2432, 'inform': 2433, 'practice': 2434, 'aggravate': 2435, 'virgin': 2436, 'muddy': 2437, 'casing': 2438, 'insert': 2439, 'glued': 2440, 'slid': 2441, 'plantronincs': 2442, 'continues': 2443, 'disapointing': 2444, 'fourth': 2445, 'fixes': 2446, 'accessing': 2447, 'downloading': 2448, 'performing': 2449, 'functions': 2450, 'constantly': 2451, 'happening': 2452, 'adapters': 2453, 'procedures': 2454, 'wiping': 2455, 'strength': 2456, 'louder': 2457, 'navigate': 2458, 'recessed': 2459, 'avoiding': 2460, 'smoking': 2461, 'linked': 2462, 'possesed': 2463, 'research': 2464, 'division': 2465, 'infuriate': 2466, 'walkman': 2467, 'charges': 2468, 'europe': 2469, 'asia': 2470, 'clipping': 2471, 'deffinitely': 2472, 'cent': 2473, 'behing': 2474, 'comfortible': 2475, 'pain': 2476, 'arrival': 2477, 'fraction': 2478, 'crappy': 2479, 'seeen': 2480, 'interface': 2481, 'decade': 2482, 'compete': 2483, 'designs': 2484, 'w': 2485, 'transceiver': 2486, 'steer': 2487, 'replacementr': 2488, 'pens': 2489, 'buyit': 2490, 'beats': 2491, 'fingers': 2492, 'steep': 2493, 'haul': 2494, 'dissapointing': 2495, 'originally': 2496, 'discarded': 2497, 'posted': 2498, 'detailed': 2499, 'grey': 2500, 'existing': 2501, 'cds': 2502, 'currently': 2503, 'shooters': 2504, 'delay': 2505, 'messes': 2506, 'bitpim': 2507, 'program': 2508, 'transfer': 2509, 'accessory': 2510, 'manufacturer': 2511, 'performed': 2512, 'muffled': 2513, 'incoming': 2514, 'severe': 2515, 'resistant': 2516, 'contacted': 2517, 'receipt': 2518, 'linksys': 2519, 'refurb': 2520, 'snug': 2521, 'heavy': 2522, 'promised': 2523, 'loop': 2524, 'latch': 2525, 'visor': 2526, 'address': 2527, 'reboots': 2528, 'tungsten': 2529, 'flipphones': 2530, 'sins': 2531, 'industrial': 2532, 'tracking': 2533, 'detachable': 2534, 'periodically': 2535, 'upload': 2536, 'locks': 2537, 'screens': 2538, 'randomly': 2539, 'locked': 2540, 'worn': 2541, 'ringer': 2542, 'acceptable': 2543, 'upbeat': 2544, 'forgeries': 2545, 'abound': 2546, 'ca': 2547, 'smallest': 2548, 'stays': 2549, 'drains': 2550, 'superfast': 2551, 'ergonomic': 2552, 'theory': 2553, 'clips': 2554, 'distracting': 2555, 'cbr': 2556, 'mps': 2557, 'preferably': 2558, 'windows': 2559, 'media': 2560, 'sos': 2561, 'signals': 2562, 'connect': 2563, 'near': 2564, 'startac': 2565, 'regretted': 2566, 'outperform': 2567, 'china': 2568, 'sim': 2569, 'replaced': 2570, 'connecting': 2571, 'sources': 2572, 'imac': 2573, 'external': 2574, 'bells': 2575, 'whistles': 2576, 'slide': 2577, 'grip': 2578, 'prevents': 2579, 'slipping': 2580, 'exclaim': 2581, 'whoa': 2582, 'corded': 2583, 'functional': 2584, 'soft': 2585, 'tight': 2586, 'shape': 2587, 'copier': 2588, 'sent': 2589, 'anywhere': 2590, 'sold': 2591, 'units': 2592, 'krussel': 2593, 'tracfonewebsite': 2594, 'toactivate': 2595, 'texas': 2596, 'dit': 2597, 'blueant': 2598, 'supertooth': 2599, 'pcs': 2600, 'sch': 2601, 'slider': 2602, 'premium': 2603, 'plugs': 2604, 'capacity': 2605, 'confortable': 2606, 'periods': 2607, 'ant': 2608, 'hey': 2609, 'pleasantly': 2610, 'suprised': 2611, 'dustpan': 2612, 'indoors': 2613, 'disposable': 2614, 'puff': 2615, 'ride': 2616, 'smoother': 2617, 'nano': 2618, 'dissapointed': 2619, 'reccommend': 2620, 'anti': 2621, 'glare': 2622, 'smartphone': 2623, 'atleast': 2624, 'amp': 2625, 'reoccure': 2626, 'somewhere': 2627, 'creaks': 2628, 'floor': 2629, 'apartment': 2630, 'inconspicuous': 2631, 'slowly': 2632, 'upgrade': 2633, 'securly': 2634, 'possibility': 2635, 'booking': 2636, 'entertainment': 2637, 'communication': 2638, 'activesync': 2639, 'optimal': 2640, 'synchronization': 2641, 'coupon': 2642, 'instance': 2643, 'cheapy': 2644, 'shouting': 2645, 'shiny': 2646, 'grtting': 2647, 'vc': 2648, 'exceeds': 2649, 'sight': 2650, 'improper': 2651, 'palms': 2652, 'hoped': 2653, 'pads': 2654, 'stops': 2655, 'intermittently': 2656, 'reaching': 2657, 'keys': 2658, 'nightmare': 2659, 'speakerphone': 2660, 'cassette': 2661, 'cellphones': 2662, 'planning': 2663, 'products': 2664, 'sensor': 2665, 'reliability': 2666, 'beeping': 2667, 'dieing': 2668, 'ir': 2669, 'cancellation': 2670, 'counterfeit': 2671, 'travled': 2672, 'swivel': 2673, 'dual': 2674, 'keeping': 2675, 'bottowm': 2676, 'gimmick': 2677, 'opens': 2678, 'causing': 2679, 'trust': 2680, 'maintains': 2681, 'flawless': 2682, 'devices': 2683, 'holder': 2684, 'land': 2685, 'loops': 2686, 'owning': 2687, 'official': 2688, 'oem': 2689, 'loudest': 2690, 'competitors': 2691, 'saved': 2692, 'alot': 2693, 'unintelligible': 2694, 'restart': 2695, 'bend': 2696, 'leaf': 2697, 'metal': 2698, 'stress': 2699, 'leopard': 2700, 'print': 2701, 'saggy': 2702, 'floppy': 2703, 'looses': 2704, 'snap': 2705, 'fliptop': 2706, 'wobbly': 2707, 'eventually': 2708, 'fulfills': 2709, 'requirements': 2710, 'rests': 2711, 'websites': 2712, 'cables': 2713, 'lap': 2714, 'controls': 2715, 'accessable': 2716, 'mine': 2717, 'satisifed': 2718, 'sa': 2719, 'specs': 2720, 'armband': 2721, 'allot': 2722, 'clearer': 2723, 'keypads': 2724, 'reach': 2725, 'ericson': 2726, 'za': 2727, 'motor': 2728, 'center': 2729, 'voltage': 2730, 'humming': 2731, 'equipment': 2732, 'wake': 2733, 'styling': 2734, 'restocking': 2735, 'fee': 2736, 'darn': 2737, 'sweetest': 2738, 'securely': 2739, 'hook': 2740, 'canal': 2741, 'videos': 2742, 'negatively': 2743, 'adapter': 2744, 'provide': 2745, 'assumed': 2746, 'lense': 2747, 'text': 2748, 'messaging': 2749, 'tricky': 2750, 'lasted': 2751, 'blew': 2752, 'flops': 2753, 'smudged': 2754, 'infra': 2755, 'port': 2756, 'irda': 2757, 'aimless': 2758, 'distressed': 2759, 'drifting': 2760, 'attempting': 2761, 'artiness': 2762, 'existent': 2763, 'gerardo': 2764, 'emptiness': 2765, 'science': 2766, 'teacher': 2767, 'owls': 2768, 'florida': 2769, 'muppets': 2770, 'overdue': 2771, 'screenplay': 2772, 'post': 2773, 'structure': 2774, 'tightly': 2775, 'vitally': 2776, 'occurs': 2777, 'content': 2778, 'superlative': 2779, 'require': 2780, 'puzzle': 2781, 'solving': 2782, 'pulls': 2783, 'punches': 2784, 'insane': 2785, 'unlockable': 2786, 'aye': 2787, 'doomed': 2788, 'conception': 2789, 'minor': 2790, 'confirm': 2791, 'exaggerating': 2792, 'trailer': 2793, 'carrell': 2794, 'co': 2795, 'freeman': 2796, 'helms': 2797, 'animals': 2798, 'integration': 2799, 'translate': 2800, 'succeeds': 2801, 'meagre': 2802, 'cancan': 2803, 'boasts': 2804, 'cutest': 2805, 'leading': 2806, 'heels': 2807, 'insipid': 2808, 'efforts': 2809, 'heche': 2810, 'sam': 2811, 'shepard': 2812, 'gung': 2813, 'ho': 2814, 'marine': 2815, 'sobering': 2816, 'riveted': 2817, 'resounding': 2818, 'hanks': 2819, 'fingernails': 2820, 'chalkboard': 2821, 'unnecessary': 2822, 'train': 2823, 'roller': 2824, 'coaster': 2825, 'grates': 2826, 'nerves': 2827, 'improved': 2828, 'improvisation': 2829, 'worry': 2830, 'surroundings': 2831, 'crackles': 2832, 'youthful': 2833, 'concentrate': 2834, 'meanders': 2835, 'builders': 2836, 'cross': 2837, 'cliche': 2838, 'predict': 2839, 'verbatim': 2840, 'chills': 2841, 'nevsky': 2842, 'whoever': 2843, 'planned': 2844, 'dodge': 2845, 'stratus': 2846, 'makers': 2847, 'restrained': 2848, 'qubec': 2849, 'promote': 2850, 'suited': 2851, 'horrendous': 2852, 'retarded': 2853, 'girls': 2854, 'manna': 2855, 'evaluate': 2856, 'veteran': 2857, 'nostalgia': 2858, 'ursula': 2859, 'burton': 2860, 'nun': 2861, 'nuns': 2862, 'shirley': 2863, 'jones': 2864, 'rendition': 2865, 'cotton': 2866, 'unfaithful': 2867, 'gere': 2868, 'dialogs': 2869, 'shallow': 2870, 'insincere': 2871, 'politically': 2872, 'savant': 2873, 'peaking': 2874, 'columbo': 2875, 'neighbour': 2876, 'misplace': 2877, 'weaker': 2878, 'debated': 2879, 'sack': 2880, 'trumpeter': 2881, 'falsely': 2882, 'murder': 2883, 'applause': 2884, 'prelude': 2885, 'villain': 2886, 'michael': 2887, 'ironside': 2888, 'rocked': 2889, 'social': 2890, 'physical': 2891, 'outlets': 2892, 'ben': 2893, 'affleck': 2894, 'leaving': 2895, 'knocked': 2896, 'excruciatingly': 2897, 'wedding': 2898, 'agreed': 2899, 'aired': 2900, 'dribble': 2901, 'imdb': 2902, 'mirrormask': 2903, 'inexperience': 2904, 'meant': 2905, 'ineptly': 2906, 'stereotypically': 2907, 'schoolers': 2908, 'cry': 2909, 'relate': 2910, 'sharing': 2911, 'confuses': 2912, 'underwater': 2913, 'repeated': 2914, 'thousand': 2915, 'redeemed': 2916, 'mstk': 2917, 'fodder': 2918, 'anthony': 2919, 'horse': 2920, 'walk': 2921, 'theatre': 2922, 'relief': 2923, 'pillow': 2924, 'tickets': 2925, 'identifies': 2926, 'haggis': 2927, 'handle': 2928, 'strokes': 2929, 'storytelling': 2930, 'painted': 2931, 'crayons': 2932, 'crash': 2933, 'provokes': 2934, 'emotion': 2935, 'teaches': 2936, 'prejudice': 2937, 'empowerment': 2938, 'shut': 2939, 'primal': 2940, 'impulse': 2941, 'preservation': 2942, 'shattered': 2943, 'recover': 2944, 'joins': 2945, 'revenge': 2946, 'boogeyman': 2947, 'zombiez': 2948, 'hellish': 2949, 'trinity': 2950, 'distinction': 2951, 'artless': 2952, 'endlessly': 2953, 'ugliest': 2954, 'merit': 2955, 'akin': 2956, 'atrocity': 2957, 'voyage': 2958, 'discovery': 2959, 'unrecommended': 2960, 'ms': 2961, 'garbo': 2962, 'bat': 2963, 'talents': 2964, 'netflix': 2965, 'stocking': 2966, 'renowned': 2967, 'frances': 2968, 'marion': 2969, 'suffered': 2970, 'heroes': 2971, 'shined': 2972, 'senior': 2973, 'showcasing': 2974, 'punched': 2975, 'gallon': 2976, 'spew': 2977, 'foxx': 2978, 'owns': 2979, 'spacek': 2980, 'coal': 2981, 'miner': 2982, 'quaid': 2983, 'balls': 2984, 'legendary': 2985, 'biographical': 2986, 'musician': 2987, 'ironically': 2988, 'secondly': 2989, 'perfected': 2990, 'chase': 2991, 'pandering': 2992, 'sabotages': 2993, 'rumbles': 2994, 'desperately': 2995, 'depending': 2996, 'logic': 2997, 'unremarkable': 2998, 'author': 2999, 'abstruse': 3000, 'reenactments': 3001, 'emotionally': 3002, 'adrift': 3003, 'stagy': 3004, 'sits': 3005, 'soldiers': 3006, 'masculinity': 3007, 'pledge': 3008, 'hairsplitting': 3009, 'purity': 3010, 'admiration': 3011, 'swords': 3012, 'kill': 3013, 'momentum': 3014, 'quicker': 3015, 'resume': 3016, 'amateurish': 3017, 'fascinated': 3018, 'inconsistencies': 3019, 'horrid': 3020, 'ballet': 3021, 'repertory': 3022, 'developments': 3023, 'woa': 3024, 'sappiest': 3025, 'unwatchable': 3026, 'filmography': 3027, 'site': 3028, 'intentions': 3029, 'undertone': 3030, 'fifties': 3031, 'existential': 3032, 'weariness': 3033, 'ought': 3034, 'regrettably': 3035, 'expression': 3036, 'celebration': 3037, 'patriotism': 3038, 'underlines': 3039, 'allison': 3040, 'concert': 3041, 'sequences': 3042, 'ridiculousness': 3043, 'surface': 3044, 'conceptually': 3045, 'everybody': 3046, 'fantasy': 3047, 'considered': 3048, 'understood': 3049, 'identified': 3050, 'crime': 3051, 'belmondo': 3052, 'lino': 3053, 'ventura': 3054, 'portrayals': 3055, 'detailing': 3056, 'loyalty': 3057, 'treachery': 3058, 'melville': 3059, 'manages': 3060, 'transcend': 3061, 'limitations': 3062, 'indie': 3063, 'continually': 3064, 'subverting': 3065, 'emerge': 3066, 'intense': 3067, 'crocdodile': 3068, 'believed': 3069, 'crocs': 3070, 'swamp': 3071, 'christopher': 3072, 'eccleston': 3073, 'tardis': 3074, 'continuation': 3075, 'succeeded': 3076, 'pi': 3077, 'witticisms': 3078, 'bob': 3079, 'rise': 3080, 'finale': 3081, 'kieslowski': 3082, 'amaze': 3083, 'colours': 3084, 'flag': 3085, 'connections': 3086, 'survivors': 3087, 'ferry': 3088, 'valentine': 3089, 'solidifying': 3090, 'happiness': 3091, 'dealt': 3092, 'roth': 3093, 'pearls': 3094, 'awarded': 3095, 'eloquently': 3096, 'francis': 3097, 'ford': 3098, 'gradually': 3099, 'develop': 3100, 'lies': 3101, 'nicola': 3102, 'roeg': 3103, 'wih': 3104, 'translating': 3105, 'strives': 3106, 'greatness': 3107, 'muddled': 3108, 'disparate': 3109, 'accurately': 3110, 'defined': 3111, 'tolerate': 3112, 'political': 3113, 'incorrectness': 3114, 'artistic': 3115, 'suspension': 3116, 'slavic': 3117, 'outlandish': 3118, 'psychotic': 3119, 'lovable': 3120, 'enjoyment': 3121, 'repair': 3122, 'bitchy': 3123, 'boss': 3124, 'tear': 3125, 'malta': 3126, 'settings': 3127, 'barren': 3128, 'hockey': 3129, 'defensemen': 3130, 'goalies': 3131, 'diving': 3132, 'net': 3133, 'superficial': 3134, 'stagey': 3135, 'farce': 3136, 'pyromaniac': 3137, 'waylaid': 3138, 'switched': 3139, 'yawn': 3140, 'educational': 3141, 'barney': 3142, 'duper': 3143, 'babie': 3144, 'bop': 3145, 'storm': 3146, 'trooper': 3147, 'potted': 3148, 'plants': 3149, 'shameful': 3150, 'television': 3151, 'writers': 3152, 'smack': 3153, 'bonus': 3154, 'episodes': 3155, 'latched': 3156, 'endearing': 3157, 'sibling': 3158, 'bond': 3159, 'unneeded': 3160, 'controversy': 3161, 'damian': 3162, 'versatile': 3163, 'cutting': 3164, 'passion': 3165, 'ireland': 3166, 'horrified': 3167, 'sympathetic': 3168, 'movements': 3169, 'shelf': 3170, 'elias': 3171, 'koteas': 3172, 'palance': 3173, 'angelina': 3174, 'naked': 3175, 'cameo': 3176, 'sven': 3177, 'ole': 3178, 'thorsen': 3179, 'ebay': 3180, 'poler': 3181, 'fort': 3182, 'steele': 3183, 'unrealistic': 3184, 'bell': 3185, 'higher': 3186, 'sink': 3187, 'slackers': 3188, 'excuses': 3189, 'actions': 3190, 'excerpts': 3191, 'stylized': 3192, 'exemplars': 3193, 'designer': 3194, 'powerful': 3195, 'explorations': 3196, 'politics': 3197, 'rendering': 3198, 'imperial': 3199, 'faultless': 3200, 'underappreciated': 3201, 'keith': 3202, 'bully': 3203, 'teddy': 3204, 'vivid': 3205, 'sean': 3206, 'noble': 3207, 'brigand': 3208, 'candace': 3209, 'bergen': 3210, 'feisty': 3211, 'heroine': 3212, 'wily': 3213, 'hay': 3214, 'kanaly': 3215, 'spiffy': 3216, 'radiant': 3217, 'lieutenant': 3218, 'roosevelt': 3219, 'stick': 3220, 'adventure': 3221, 'ryans': 3222, 'portrayed': 3223, 'schizophrenic': 3224, 'affected': 3225, 'apt': 3226, 'imitation': 3227, 'individual': 3228, 'fashioned': 3229, 'tuneful': 3230, 'rita': 3231, 'hayworth': 3232, 'pedestal': 3233, 'exploit': 3234, 'financial': 3235, 'gain': 3236, 'linear': 3237, 'narration': 3238, 'flashbacks': 3239, 'articulated': 3240, 'monica': 3241, 'bellucci': 3242, 'commentary': 3243, 'timers': 3244, 'laughs': 3245, 'northern': 3246, 'community': 3247, 'represents': 3248, 'vivian': 3249, 'schilling': 3250, 'theatres': 3251, 'screamy': 3252, 'masculine': 3253, 'casted': 3254, 'throwback': 3255, 'student': 3256, 'experiences': 3257, 'abroad': 3258, 'interacting': 3259, 'nationalities': 3260, 'slightly': 3261, 'ticker': 3262, 'buildings': 3263, 'locations': 3264, 'hummh': 3265, 'yelps': 3266, 'hes': 3267, 'funniest': 3268, 'curtain': 3269, 'ive': 3270, 'edition': 3271, 'lid': 3272, 'romantic': 3273, 'laughed': 3274, 'chosen': 3275, 'tortured': 3276, 'blatant': 3277, 'propaganda': 3278, 'hatred': 3279, 'admitted': 3280, 'school': 3281, 'admins': 3282, 'thunderbirds': 3283, 'motion': 3284, 'needlessly': 3285, 'repeats': 3286, 'backed': 3287, 'vehicles': 3288, 'flakes': 3289, 'bordered': 3290, 'stupidity': 3291, 'filmmaker': 3292, 'hide': 3293, 'monumental': 3294, 'halfway': 3295, 'embarrassed': 3296, 'howell': 3297, 'bag': 3298, 'stephen': 3299, 'mchattie': 3300, 'lance': 3301, 'hendrikson': 3302, 'raging': 3303, 'cheekbones': 3304, 'professionals': 3305, 'debbie': 3306, 'rochon': 3307, 'contributory': 3308, 'former': 3309, 'chimp': 3310, 'tolerable': 3311, 'oy': 3312, 'vey': 3313, 'simplifying': 3314, 'brevity': 3315, 'fulfilling': 3316, 'amusing': 3317, 'convoluted': 3318, 'convince': 3319, 'questioning': 3320, 'bela': 3321, 'lugosi': 3322, 'extraneous': 3323, 'intoning': 3324, 'decidely': 3325, 'universal': 3326, 'armand': 3327, 'assante': 3328, 'tying': 3329, 'comprehensible': 3330, 'hang': 3331, 'camp': 3332, 'expecting': 3333, 'iq': 3334, 'mollusk': 3335, 'loosely': 3336, 'string': 3337, 'lower': 3338, 'scripts': 3339, 'victor': 3340, 'mclaglen': 3341, 'donlevy': 3342, 'president': 3343, 'u': 3344, 'unmoving': 3345, 'producer': 3346, 'gifted': 3347, 'cheerless': 3348, 'heist': 3349, 'characterisation': 3350, 'underbite': 3351, 'stoic': 3352, 'emoting': 3353, 'yun': 3354, 'tomorrow': 3355, 'abandoned': 3356, 'factory': 3357, 'executed': 3358, 'flying': 3359, 'judo': 3360, 'la': 3361, 'woo': 3362, 'squibs': 3363, 'derivative': 3364, 'hopeless': 3365, 'uneasy': 3366, 'composed': 3367, 'elderly': 3368, 'babbling': 3369, 'overwrought': 3370, 'pseudo': 3371, 'satanic': 3372, 'gibberish': 3373, 'corny': 3374, 'teen': 3375, 'goth': 3376, 'blush': 3377, 'olde': 3378, 'latin': 3379, 'sloppy': 3380, 'underacting': 3381, 'competent': 3382, 'jerry': 3383, 'falwell': 3384, 'starring': 3385, 'jaclyn': 3386, 'smith': 3387, 'lifetime': 3388, 'sells': 3389, 'assaulted': 3390, 'strident': 3391, 'cords': 3392, 'blare': 3393, 'pap': 3394, 'screened': 3395, 'punish': 3396, 'unemployed': 3397, 'understatement': 3398, 'universe': 3399, 'team': 3400, 'trond': 3401, 'fausa': 3402, 'aurvg': 3403, 'bothersome': 3404, 'comment': 3405, 'move': 3406, 'initially': 3407, 'local': 3408, 'sites': 3409, 'buffalo': 3410, 'intrigued': 3411, 'applauded': 3412, 'lilt': 3413, 'duris': 3414, 'views': 3415, 'barcelona': 3416, 'famed': 3417, 'gaudi': 3418, 'towers': 3419, 'despised': 3420, 'rough': 3421, 'draft': 3422, 'shooting': 3423, 'completed': 3424, 'personalities': 3425, 'twists': 3426, 'occur': 3427, 'irritating': 3428, 'indescribably': 3429, 'idiotic': 3430, 'cheesiness': 3431, 'unethical': 3432, 'nc': 3433, 'zillion': 3434, 'washing': 3435, 'twirling': 3436, 'grade': 3437, 'z': 3438, 'road': 3439, 'silly': 3440, 'unmitigated': 3441, 'blandly': 3442, 'edward': 3443, 'chodorov': 3444, 'negulesco': 3445, 'widmark': 3446, 'unintentionally': 3447, 'comical': 3448, 'marbles': 3449, 'marred': 3450, 'constant': 3451, 'exteriors': 3452, 'prompted': 3453, 'aged': 3454, 'upper': 3455, 'uptight': 3456, 'mistakes': 3457, 'jokes': 3458, 'offend': 3459, 'littered': 3460, 'overt': 3461, 'slurs': 3462, 'members': 3463, 'whites': 3464, 'depicted': 3465, 'morons': 3466, 'boobs': 3467, 'lets': 3468, 'destroy': 3469, 'latifa': 3470, 'wouldnt': 3471, 'steamboat': 3472, 'willie': 3473, 'amazingly': 3474, 'plane': 3475, 'tons': 3476, 'granted': 3477, 'timeless': 3478, 'straw': 3479, 'cruel': 3480, 'bipolarity': 3481, 'thug': 3482, 'luv': 3483, 'diaper': 3484, 'commercial': 3485, 'awards': 3486, 'accolades': 3487, 'body': 3488, 'garfield': 3489, 'ann': 3490, 'revere': 3491, 'lilli': 3492, 'plmer': 3493, 'william': 3494, 'conrad': 3495, 'lee': 3496, 'cinematographers': 3497, 'wong': 3498, 'howe': 3499, 'qualities': 3500, 'thomerson': 3501, 'professor': 3502, 'established': 3503, 'students': 3504, 'necklace': 3505, 'containing': 3506, 'meteorite': 3507, 'matthews': 3508, 'wrote': 3509, 'grim': 3510, 'aspects': 3511, 'gore': 3512, 'convention': 3513, 'cg': 3514, 'slideshow': 3515, 'explosion': 3516, 'tanks': 3517, 'okay': 3518, 'critic': 3519, 'creature': 3520, 'gotta': 3521, 'slimy': 3522, 'drooling': 3523, 'breeders': 3524, 'ebola': 3525, 'virus': 3526, 'comedic': 3527, 'timing': 3528, 'debits': 3529, 'popcorn': 3530, 'borders': 3531, 'lewis': 3532, 'considerable': 3533, 'incendiary': 3534, 'unrestrained': 3535, 'sole': 3536, 'bright': 3537, 'superbad': 3538, 'interim': 3539, 'originality': 3540, 'freshness': 3541, 'offer': 3542, 'evidently': 3543, 'reflected': 3544, 'borrowed': 3545, 'filmiing': 3546, 'expansive': 3547, 'convey': 3548, 'broad': 3549, 'sweep': 3550, 'landscapes': 3551, 'trashy': 3552, 'precisely': 3553, 'proudly': 3554, 'classical': 3555, 'wb': 3556, 'toons': 3557, 'delivered': 3558, 'underrated': 3559, 'murky': 3560, 'related': 3561, 'sitcoms': 3562, 'oriented': 3563, 'teenagers': 3564, 'peculiarity': 3565, 'excessively': 3566, 'phony': 3567, 'contrived': 3568, 'um': 3569, 'perabo': 3570, 'foolish': 3571, 'accents': 3572, 'hypocrisy': 3573, 'rubbish': 3574, 'researched': 3575, 'pitch': 3576, 'omit': 3577, 'instant': 3578, 'catchy': 3579, 'credits': 3580, 'simmering': 3581, 'boiling': 3582, 'warts': 3583, 'poet': 3584, 'bohemian': 3585, 'wartime': 3586, 'bombardments': 3587, 'london': 3588, 'outward': 3589, 'tranquillity': 3590, 'welsh': 3591, 'coastal': 3592, 'retreat': 3593, 'borderlines': 3594, 'friendship': 3595, 'lust': 3596, 'concerns': 3597, 'jealousy': 3598, 'rivalry': 3599, 'cowardice': 3600, 'egotism': 3601, 'heroism': 3602, 'sacrifice': 3603, 'suggests': 3604, 'tensions': 3605, 'inappropriate': 3606, 'smiling': 3607, 'keira': 3608, 'knightley': 3609, 'prone': 3610, 'inventive': 3611, 'pointillistic': 3612, 'behold': 3613, 'continuously': 3614, 'bertolucci': 3615, 'sidelined': 3616, 'inexplicable': 3617, 'crew': 3618, 'regardless': 3619, 'custer': 3620, 'alongside': 3621, 'olivia': 3622, 'havilland': 3623, 'dads': 3624, 'grew': 3625, 'realised': 3626, 'jim': 3627, 'connor': 3628, 'energetic': 3629, 'george': 3630, 'hosting': 3631, 'overs': 3632, 'monotonous': 3633, 'shenanigans': 3634, 'spy': 3635, 'kidnapped': 3636, 'foreign': 3637, 'random': 3638, 'taxidermists': 3639, 'stewart': 3640, 'hero': 3641, 'rips': 3642, 'climax': 3643, 'embassy': 3644, 'brooding': 3645, 'menace': 3646, 'nevertheless': 3647, 'stable': 3648, 'producers': 3649, 'scot': 3650, 'vandiver': 3651, 'nonetheless': 3652, 'campy': 3653, 'bendingly': 3654, 'scares': 3655, 'medical': 3656, 'terminology': 3657, 'iffy': 3658, 'insulin': 3659, 'dependant': 3660, 'diabetic': 3661, 'complexity': 3662, 'task': 3663, 'challenges': 3664, 'facing': 3665, 'attempted': 3666, 'truth': 3667, 'reconciliation': 3668, 'magnificent': 3669, 'woven': 3670, 'splendid': 3671, 'view': 3672, 'subtitles': 3673, 'aversion': 3674, 'therapy': 3675, 'jason': 3676, 'moved': 3677, 'tears': 3678, 'monolog': 3679, 'candle': 3680, 'sphere': 3681, 'helen': 3682, 'baxendale': 3683, 'credible': 3684, 'cheerfull': 3685, 'naughty': 3686, 'deadly': 3687, 'shakespears': 3688, 'lyrics': 3689, 'dr': 3690, 'seuss': 3691, 'brilliantly': 3692, 'animated': 3693, 'upa': 3694, 'finest': 3695, 'academy': 3696, 'award': 3697, 'integral': 3698, 'helping': 3699, 'bailey': 3700, 'eiko': 3701, 'ishioka': 3702, 'dracula': 3703, 'format': 3704, 'baaaaaad': 3705, 'collect': 3706, 'extant': 3707, 'austen': 3708, 'consolations': 3709, 'speaking': 3710, 'unbearably': 3711, 'kitchy': 3712, 'raw': 3713, 'sublimely': 3714, 'theatrical': 3715, 'terror': 3716, 'lie': 3717, 'escalating': 3718, 'monstrous': 3719, 'consequences': 3720, 'mesmerising': 3721, 'fumbling': 3722, 'hankies': 3723, 'faces': 3724, 'males': 3725, 'females': 3726, 'alike': 3727, 'julian': 3728, 'fellowes': 3729, 'triumphed': 3730, 'national': 3731, 'treasure': 3732, 'phrase': 3733, 'owed': 3734, 'kris': 3735, 'kristoffersen': 3736, 'marriage': 3737, 'errors': 3738, 'commented': 3739, 'cars': 3740, 'garage': 3741, 'facial': 3742, 'configuration': 3743, 'inspiration': 3744, 'overcome': 3745, 'rejection': 3746, 'punishment': 3747, 'park': 3748, 'southern': 3749, 'california': 3750, 'desert': 3751, 'patent': 3752, 'accessible': 3753, 'miserable': 3754, 'angela': 3755, 'bennett': 3756, 'pretext': 3757, 'repeating': 3758, 'robotic': 3759, 'moves': 3760, 'delete': 3761, 'uncalled': 3762, 'geek': 3763, 'bible': 3764, 'thumper': 3765, 'pulling': 3766, 'equivalent': 3767, 'dickens': 3768, 'carol': 3769, 'sensibility': 3770, 'success': 3771, 'depends': 3772, 'sydney': 3773, 'greenstreet': 3774, 'yardley': 3775, 'pleasing': 3776, 'rpg': 3777, 'respecting': 3778, 'rpger': 3779, 'youtube': 3780, 'lassie': 3781, 'painfully': 3782, 'dreary': 3783, 'waster': 3784, 'becomes': 3785, 'forgettable': 3786, 'shell': 3787, 'barking': 3788, 'transfers': 3789, 'relaxing': 3790, 'title': 3791, 'handles': 3792, 'dignity': 3793, 'shocking': 3794, 'memorized': 3795, 'fishnet': 3796, 'stockings': 3797, 'astonishingly': 3798, 'ham': 3799, 'fisted': 3800, 'irons': 3801, 'unconditional': 3802, 'ordeal': 3803, 'progresses': 3804, 'anguish': 3805, 'pans': 3806, 'reviewers': 3807, 'interview': 3808, 'vampire': 3809, 'lestat': 3810, 'stuart': 3811, 'townsend': 3812, 'cruise': 3813, 'aailiyah': 3814, 'akasha': 3815, 'entrance': 3816, 'sensitivities': 3817, 'treatments': 3818, 'details': 3819, 'dysfunction': 3820, 'football': 3821, 'perplexing': 3822, 'skilled': 3823, 'meredith': 3824, 'maker': 3825, 'worthy': 3826, 'syrupy': 3827, 'nine': 3828, 'goremeister': 3829, 'lucio': 3830, 'thrillers': 3831, 'standout': 3832, 'unpleasant': 3833, 'technically': 3834, 'riz': 3835, 'ortolani': 3836, 'recurring': 3837, 'unaccompanied': 3838, 'vocal': 3839, 'distant': 3840, 'dustin': 3841, 'hoffman': 3842, 'involving': 3843, 'issue': 3844, 'tired': 3845, 'jennifer': 3846, 'rubin': 3847, 'harris': 3848, 'changes': 3849, 'nervous': 3850, 'starlet': 3851, 'coach': 3852, 'snow': 3853, 'ultra': 3854, 'exciting': 3855, 'uniqueness': 3856, 'foreigner': 3857, 'stranger': 3858, 'considers': 3859, 'ussr': 3860, 'scream': 3861, 'scare': 3862, 'indictment': 3863, 'justice': 3864, 'system': 3865, 'fundamental': 3866, 'legal': 3867, 'guilt': 3868, 'innocence': 3869, 'routine': 3870, 'constantine': 3871, 'intensity': 3872, 'motivations': 3873, 'inspiring': 3874, 'stinker': 3875, 'direct': 3876, 'release': 3877, 'trek': 3878, 'frontier': 3879, 'includes': 3880, 'shatner': 3881, 'nimoy': 3882, 'washed': 3883, 'tacky': 3884, 'spock': 3885, 'rescue': 3886, 'kirk': 3887, 'jet': 3888, 'mountain': 3889, 'humor': 3890, 'dosen': 3891, 'involves': 3892, 'vulcan': 3893, 'stealing': 3894, 'enterprise': 3895, 'uhura': 3896, 'distract': 3897, 'guards': 3898, 'subjects': 3899, 'jay': 3900, 'adams': 3901, 'unfortunate': 3902, 'subject': 3903, 'evokes': 3904, 'grainy': 3905, 'enhanced': 3906, 'surprises': 3907, 'achille': 3908, 'philippa': 3909, 'sing': 3910, 'giovanni': 3911, 'describes': 3912, 'layers': 3913, 'jutland': 3914, 'instruments': 3915, 'general': 3916, 'loewenhielm': 3917, 'partaking': 3918, 'cailles': 3919, 'en': 3920, 'sarcophage': 3921, 'savor': 3922, 'impression': 3923, 'generates': 3924, 'austere': 3925, 'backdrop': 3926, 'wanting': 3927, 'notch': 3928, 'fleshed': 3929, 'grimes': 3930, 'blake': 3931, 'scripted': 3932, 'deadpan': 3933, 'angus': 3934, 'scrimm': 3935, 'gently': 3936, 'menacing': 3937, 'violin': 3938, 'anatomist': 3939, 'entertained': 3940, 'phantasm': 3941, 'collective': 3942, 'concerning': 3943, 'kudos': 3944, 'juano': 3945, 'hernandez': 3946, 'supporting': 3947, 'murdering': 3948, 'trumbull': 3949, 'evinced': 3950, 'faithful': 3951, 'joyce': 3952, 'acclaimed': 3953, 'novella': 3954, 'feelings': 3955, 'gabriel': 3956, 'intangibles': 3957, 'leap': 3958, 'viewer': 3959, 'grasp': 3960, 'wondered': 3961, 'shortlist': 3962, 'emperor': 3963, 'surely': 3964, 'coherent': 3965, 'force': 3966, 'obliged': 3967, 'creative': 3968, 'subplots': 3969, 'native': 3970, 'brainsucking': 3971, 'telly': 3972, 'savalas': 3973, 'boyle': 3974, 'sheer': 3975, 'tedium': 3976, 'melodrama': 3977, 'sinking': 3978, 'decisions': 3979, 'establish': 3980, 'follows': 3981, 'band': 3982, 'latter': 3983, 'mansonites': 3984, 'reporter': 3985, 'anniversary': 3986, 'killings': 3987, 'indication': 3988, 'meld': 3989, 'volatile': 3990, 'temperaments': 3991, 'seamless': 3992, 'union': 3993, 'creativity': 3994, 'powerhouse': 3995, 'fascination': 3996, 'celebrity': 3997, 'interpretations': 3998, 'fame': 3999, 'forgotten': 4000, 'kevin': 4001, 'spacey': 4002, 'verbal': 4003, 'tsunami': 4004, 'buddy': 4005, 'ackerman': 4006, 'scripting': 4007, 'unmatched': 4008, 'ages': 4009, 'younger': 4010, 'galley': 4011, 'forwarded': 4012, 'horrendously': 4013, 'captain': 4014, 'howdy': 4015, 'semi': 4016, 'truck': 4017, 'linda': 4018, 'cardellini': 4019, 'poised': 4020, 'dee': 4021, 'snider': 4022, 'villains': 4023, 'sophisticated': 4024, 'miserably': 4025, 'unoriginal': 4026, 'cinematic': 4027, 'captured': 4028, 'celluloid': 4029, 'supernatural': 4030, 'thread': 4031, 'leni': 4032, 'parker': 4033, 'anita': 4034, 'laselva': 4035, 'taelons': 4036, 'idealogical': 4037, 'centers': 4038, 'puppet': 4039, 'flicks': 4040, 'explains': 4041, 'sinister': 4042, 'origins': 4043, 'f': 4044, 'x': 4045, 'insomniacs': 4046, 'heartwarming': 4047, 'chasing': 4048, 'nurse': 4049, 'unpredictability': 4050, 'labute': 4051, 'brutal': 4052, 'violence': 4053, 'seperate': 4054, 'secondary': 4055, 'incomprehensible': 4056, 'relation': 4057, 'primary': 4058, 'mystifying': 4059, 'shelves': 4060, 'warn': 4061, 'dumbest': 4062, 'hbo': 4063, 'cox': 4064, 'win': 4065, 'agree': 4066, 'jessica': 4067, 'dimensional': 4068, 'elaborately': 4069, 'aesthetically': 4070, 'sculpture': 4071, 'soap': 4072, 'operas': 4073, 'pray': 4074, 'charismatic': 4075, 'paolo': 4076, 'sorrentino': 4077, 'tony': 4078, 'unforgettable': 4079, 'titta': 4080, 'di': 4081, 'girolamo': 4082, 'vision': 4083, 'debut': 4084, 'charlie': 4085, 'kathy': 4086, 'bates': 4087, 'desperation': 4088, 'escapism': 4089, 'variation': 4090, 'fields': 4091, 'condescends': 4092, 'arts': 4093, 'daughters': 4094, 'paint': 4095, 'photograph': 4096, 'poignant': 4097, 'alert': 4098, 'decipher': 4099, 'meanings': 4100, 'matrix': 4101, 'sequels': 4102, 'mindblowing': 4103, 'dream': 4104, 'rickman': 4105, 'stowe': 4106, 'hilt': 4107, 'childrens': 4108, 'subversive': 4109, 'tract': 4110, 'gloriously': 4111, 'accurate': 4112, 'raver': 4113, 'idyllic': 4114, 'downs': 4115, 'groove': 4116, 'antithesis': 4117, 'traffic': 4118, 'crowe': 4119, 'picked': 4120, 'government': 4121, 'scientist': 4122, 'argued': 4123, 'humanity': 4124, 'dwight': 4125, 'schultz': 4126, 'sincere': 4127, 'critical': 4128, 'gorman': 4129, 'bechard': 4130, 'homework': 4131, 'appropriate': 4132, 'captures': 4133, 'essence': 4134, 'track': 4135, 'commands': 4136, 'contributing': 4137, 'threshold': 4138, 'hayao': 4139, 'eighth': 4140, 'ghibili': 4141, 'gake': 4142, 'ue': 4143, 'cliff': 4144, 'dominated': 4145, 'disney': 4146, 'pixar': 4147, 'cgi': 4148, 'comforting': 4149, 'relying': 4150, 'drawn': 4151, 'enchanting': 4152, 'remarkable': 4153, 'crayon': 4154, 'pencil': 4155, 'drawings': 4156, 'colorful': 4157, 'fanciful': 4158, 'drift': 4159, 'surf': 4160, 'impact': 4161, 'armageddon': 4162, 'choked': 4163, 'vessel': 4164, 'taken': 4165, 'mighty': 4166, 'frost': 4167, 'bonuses': 4168, 'fest': 4169, 'spoiled': 4170, 'brat': 4171, 'babysitting': 4172, 'sundays': 4173, 'march': 4174, 'judith': 4175, 'cutie': 4176, 'confidence': 4177, 'riot': 4178, 'hugo': 4179, 'weaving': 4180, 'obsessed': 4181, 'gay': 4182, 'estate': 4183, 'salesman': 4184, 'clients': 4185, 'houses': 4186, 'trysts': 4187, 'flaming': 4188, 'darren': 4189, 'hollander': 4190, 'flowed': 4191, 'bonding': 4192, 'hoot': 4193, 'n': 4194, 'jessice': 4195, 'clothes': 4196, 'virtue': 4197, 'regrettable': 4198, 'exceptionally': 4199, 'bank': 4200, 'holiday': 4201, 'rick': 4202, 'honeslty': 4203, 'ahead': 4204, 'warmer': 4205, 'wayyy': 4206, 'cape': 4207, 'cod': 4208, 'ravoli': 4209, 'cranberry': 4210, 'disgusted': 4211, 'shocked': 4212, 'indicate': 4213, 'cash': 4214, 'burrittos': 4215, 'interior': 4216, 'perform': 4217, 'velvet': 4218, 'cake': 4219, 'ohhh': 4220, 'hole': 4221, 'luke': 4222, 'sever': 4223, 'combos': 4224, 'accident': 4225, 'grab': 4226, 'pub': 4227, 'hiro': 4228, 'melted': 4229, 'styrofoam': 4230, 'pucks': 4231, 'disgust': 4232, 'register': 4233, 'rib': 4234, 'section': 4235, 'firehouse': 4236, 'pink': 4237, 'mein': 4238, 'lined': 4239, 'strings': 4240, 'banana': 4241, 'petrified': 4242, 'receives': 4243, 'cocktails': 4244, 'handmade': 4245, 'military': 4246, 'dos': 4247, 'gringos': 4248, 'tastings': 4249, 'jeff': 4250, 'milkshake': 4251, 'chocolate': 4252, 'milk': 4253, 'excalibur': 4254, 'common': 4255, 'cheated': 4256, 'experiencing': 4257, 'parties': 4258, 'smelled': 4259, 'pan': 4260, 'cakes': 4261, 'raving': 4262, 'tailored': 4263, 'palate': 4264, 'ratio': 4265, 'tenders': 4266, 'unsatisfying': 4267, 'omelets': 4268, 'sexy': 4269, 'outrageously': 4270, 'flirting': 4271, 'hottest': 4272, 'rock': 4273, 'casino': 4274, 'forward': 4275, 'bone': 4276, 'bloddy': 4277, 'mussels': 4278, 'reduction': 4279, 'buffets': 4280, 'tigerlilly': 4281, 'personable': 4282, 'sooooo': 4283, 'yama': 4284, 'arriving': 4285, 'actual': 4286, 'blandest': 4287, 'cuisine': 4288, 'worries': 4289, 'loving': 4290, 'venture': 4291, 'host': 4292, 'bitches': 4293, 'liking': 4294, 'reviewing': 4295, 'venturing': 4296, 'penne': 4297, 'vodka': 4298, 'meatloaf': 4299, 'lox': 4300, 'capers': 4301, 'meet': 4302, 'weekend': 4303, 'suggestions': 4304, 'bamboo': 4305, 'shoots': 4306, 'blanket': 4307, 'moz': 4308, 'subpar': 4309, 'ignore': 4310, 'fianc': 4311, 'mandalay': 4312, 'forty': 4313, 'vain': 4314, 'crostini': 4315, 'nigiri': 4316, 'flavored': 4317, 'voodoo': 4318, 'gluten': 4319, 'leftover': 4320, 'relocated': 4321, 'diverse': 4322, 'hella': 4323, 'salty': 4324, 'spinach': 4325, 'avocado': 4326, 'ingredients': 4327, 'handed': 4328, 'listed': 4329, 'waitresses': 4330, 'lordy': 4331, 'khao': 4332, 'soi': 4333, 'accommodations': 4334, 'inspired': 4335, 'desired': 4336, 'maintaining': 4337, 'coziness': 4338, 'weekly': 4339, 'haunt': 4340, 'asking': 4341, 'verge': 4342, 'dressed': 4343, 'rudely': 4344, 'hits': 4345, 'quantity': 4346, 'lemon': 4347, 'raspberry': 4348, 'cocktail': 4349, 'imagined': 4350, 'crepe': 4351, 'bits': 4352, 'missing': 4353, 'joey': 4354, 'voted': 4355, 'magazine': 4356, 'fridays': 4357, 'blows': 4358, 'exceeding': 4359, 'dreamed': 4360, 'serivce': 4361, 'inviting': 4362, 'foot': 4363, 'mixed': 4364, 'mushrooms': 4365, 'yukon': 4366, 'beateous': 4367, 'climbing': 4368, 'tartar': 4369, 'jamaican': 4370, 'mojitos': 4371, 'rich': 4372, 'accordingly': 4373, 'rinse': 4374, 'nude': 4375, 'bussell': 4376, 'sprouts': 4377, 'risotto': 4378, 'bodes': 4379, 'wrapped': 4380, 'dates': 4381, 'otto': 4382, 'welcome': 4383, 'mains': 4384, 'uninspired': 4385, 'drunk': 4386, 'patty': 4387, 'uploaded': 4388, 'sporting': 4389, 'walls': 4390, 'descriptions': 4391, 'eel': 4392, 'sauces': 4393, 'hardest': 4394, 'rolled': 4395, 'providing': 4396, 'flavourful': 4397, 'freezing': 4398, 'ayce': 4399, 'mood': 4400, 'gratitude': 4401, 'privileged': 4402, 'silently': 4403, 'peanut': 4404, 'godfathers': 4405, 'recall': 4406, 'visited': 4407, 'proclaimed': 4408, 'wildly': 4409, 'veggitarian': 4410, 'platter': 4411, 'madison': 4412, 'ironman': 4413, 'chefs': 4414, 'dedicated': 4415, 'spots': 4416, 'jenni': 4417, 'goat': 4418, 'skimp': 4419, 'mac': 4420, 'burned': 4421, 'saganaki': 4422, 'disagree': 4423, 'fellow': 4424, 'yelpers': 4425, 'noodles': 4426, 'chip': 4427, 'count': 4428, 'greedy': 4429, 'corporation': 4430, 'dime': 4431, 'outdoor': 4432, 'muffin': 4433, 'untoasted': 4434, 'bus': 4435, 'figured': 4436, 'publicly': 4437, 'loudly': 4438, 'bbq': 4439, 'lighter': 4440, 'downside': 4441, 'shawarrrrrrma': 4442, 'eyed': 4443, 'peas': 4444, 'unreal': 4445, 'vinaigrette': 4446, 'honor': 4447, 'hut': 4448, 'coupons': 4449, 'unbelievably': 4450, 'covers': 4451, 'replenished': 4452, 'yucky': 4453, 'delicioso': 4454, 'spaghetti': 4455, 'tucson': 4456, 'chipotle': 4457, 'succulent': 4458, 'baseball': 4459, 'app': 4460, 'genuinely': 4461, 'enthusiastic': 4462, 'sadly': 4463, 'gordon': 4464, 'ramsey': 4465, 'shall': 4466, 'offered': 4467, 'handling': 4468, 'rowdy': 4469, 'despicable': 4470, 'craving': 4471, 'ache': 4472, 'elegantly': 4473, 'customize': 4474, 'bean': 4475, 'outta': 4476, 'inconsiderate': 4477, 'hi': 4478, 'dinners': 4479, 'outshining': 4480, 'halibut': 4481, 'starving': 4482, 'def': 4483, 'ethic': 4484, 'andddd': 4485, 'located': 4486, 'crystals': 4487, 'shopping': 4488, 'aria': 4489, 'summarize': 4490, 'nay': 4491, 'transcendant': 4492, 'pneumatic': 4493, 'condiment': 4494, 'dispenser': 4495, 'ians': 4496, 'kiddos': 4497, 'bouchon': 4498, 'accountant': 4499, 'screwed': 4500, 'reminds': 4501, 'shops': 4502, 'san': 4503, 'francisco': 4504, 'buldogis': 4505, 'gourmet': 4506, 'frustrated': 4507, 'petty': 4508, 'iced': 4509, 'hungry': 4510, 'sore': 4511, 'companions': 4512, 'smeared': 4513, 'tracked': 4514, 'bird': 4515, 'poop': 4516, 'furthermore': 4517, 'operation': 4518, 'connisseur': 4519, 'topic': 4520, 'jerk': 4521, 'strike': 4522, 'rushed': 4523, 'across': 4524, 'appetizer': 4525, 'absolutley': 4526, 'lb': 4527, 'ths': 4528, 'gristle': 4529, 'steiners': 4530, 'fs': 4531, 'pears': 4532, 'almonds': 4533, 'spicier': 4534, 'prefer': 4535, 'ribeye': 4536, 'mesquite': 4537, 'gooodd': 4538, 'connoisseur': 4539, 'mouthful': 4540, 'relaxed': 4541, 'venue': 4542, 'couples': 4543, 'groups': 4544, 'nargile': 4545, 'tater': 4546, 'southwest': 4547, 'vanilla': 4548, 'smooth': 4549, 'profiterole': 4550, 'choux': 4551, 'pastry': 4552, 'az': 4553, 'carly': 4554, 'ventilation': 4555, 'upgrading': 4556, 'letdown': 4557, 'camelback': 4558, 'shop': 4559, 'cartel': 4560, 'trimmed': 4561, 'claimed': 4562, 'jewel': 4563, 'las': 4564, 'boiled': 4565, 'toro': 4566, 'tartare': 4567, 'cavier': 4568, 'thinly': 4569, 'wagyu': 4570, 'truffle': 4571, 'attached': 4572, 'decide': 4573, 'humiliated': 4574, 'worker': 4575, 'name': 4576, 'callings': 4577, 'daily': 4578, 'specials': 4579, 'tragedy': 4580, 'pancake': 4581, 'crawfish': 4582, 'multi': 4583, 'grain': 4584, 'pumpkin': 4585, 'pecan': 4586, 'fluffy': 4587, 'airline': 4588, 'noca': 4589, 'gyro': 4590, 'lettuce': 4591, 'pastas': 4592, 'cheesecurds': 4593, 'finger': 4594, 'beauty': 4595, 'greasy': 4596, 'unhealthy': 4597, 'similarly': 4598, 'apology': 4599, 'tiramisu': 4600, 'cannoli': 4601, 'sun': 4602, 'meats': 4603, 'frenchman': 4604, 'martini': 4605, 'gc': 4606, 'thirty': 4607, 'vacant': 4608, 'yellowtail': 4609, 'carpaccio': 4610, 'strangers': 4611, 'hello': 4612, 'donut': 4613, 'saving': 4614, 'disgraceful': 4615, 'suffers': 4616, 'greens': 4617, 'hankering': 4618, 'theft': 4619, 'eew': 4620, 'overhaul': 4621, 'witnessed': 4622, 'regularly': 4623, 'swung': 4624, 'efficient': 4625, 'sucker': 4626, 'olives': 4627, 'perpared': 4628, 'giant': 4629, 'slices': 4630, 'dusted': 4631, 'powdered': 4632, 'sugar': 4633, 'fo': 4634, 'accomodate': 4635, 'vegan': 4636, 'veggie': 4637, 'crumby': 4638, 'pale': 4639, 'croutons': 4640, 'trips': 4641, 'crema': 4642, 'caf': 4643, 'expanded': 4644, 'philadelphia': 4645, 'north': 4646, 'scottsdale': 4647, 'soooooo': 4648, 'freaking': 4649, 'papers': 4650, 'reheated': 4651, 'wedges': 4652, 'absolute': 4653, 'bloody': 4654, 'businesses': 4655, 'yellow': 4656, 'saffron': 4657, 'seasoning': 4658, 'grandmother': 4659, 'ignored': 4660, 'hostess': 4661, 'boys': 4662, 'shirt': 4663, 'drastically': 4664, 'caesar': 4665, 'madhouse': 4666, 'proven': 4667, 'moods': 4668, 'macarons': 4669, 'insanely': 4670, 'informative': 4671, 'plater': 4672, 'relax': 4673, 'screams': 4674, 'somethat': 4675, 'duo': 4676, 'violinists': 4677, 'requested': 4678, 'baklava': 4679, 'falafels': 4680, 'baba': 4681, 'ganoush': 4682, 'mgm': 4683, 'courteous': 4684, 'eclectic': 4685, 'onion': 4686, 'rings': 4687, 'nobu': 4688, 'google': 4689, 'smashburger': 4690, 'lover': 4691, 'plantains': 4692, 'spends': 4693, 'panna': 4694, 'cotta': 4695, 'flavors': 4696, 'slaw': 4697, 'drenched': 4698, 'piano': 4699, 'rge': 4700, 'fillet': 4701, 'relleno': 4702, 'sergeant': 4703, 'auju': 4704, 'hawaiian': 4705, 'breeze': 4706, 'mango': 4707, 'magic': 4708, 'pineapple': 4709, 'smoothies': 4710, 'mortify': 4711, 'anyways': 4712, 'dripping': 4713, 'hospitality': 4714, 'paradise': 4715, 'refrained': 4716, 'recommending': 4717, 'cibo': 4718, 'mouths': 4719, 'bellies': 4720, 'thumb': 4721, 'dough': 4722, 'elk': 4723, 'hooked': 4724, 'classics': 4725, 'sorely': 4726, 'quaint': 4727, 'deliciously': 4728, 'dylan': 4729, 'tummy': 4730, 'gratuity': 4731, 'larger': 4732, 'apple': 4733, 'han': 4734, 'nan': 4735, 'edinburgh': 4736, 'revisiting': 4737, 'naan': 4738, 'pine': 4739, 'touched': 4740, 'airport': 4741, 'speedy': 4742, 'calligraphy': 4743, 'stood': 4744, 'guest': 4745, 'extensive': 4746, 'inflate': 4747, 'smaller': 4748, 'grow': 4749, 'rapidly': 4750, 'lil': 4751, 'fuzzy': 4752, 'wontons': 4753, 'spice': 4754, 'whelm': 4755, 'arepas': 4756, 'jalapeno': 4757, 'shoe': 4758, 'block': 4759, 'fancy': 4760, 'affordable': 4761, 'soups': 4762, 'sunday': 4763, 'hunan': 4764, 'flair': 4765, 'bartenders': 4766, 'nutshell': 4767, 'restaraunt': 4768, 'sewer': 4769, 'veal': 4770, 'satifying': 4771, 'join': 4772, 'email': 4773, 'colder': 4774, 'describing': 4775, 'tepid': 4776, 'chains': 4777, 'crowds': 4778, 'juries': 4779, 'lawyers': 4780, 'arrives': 4781, 'paying': 4782, 'wienerschnitzel': 4783, 'maine': 4784, 'roll': 4785, 'law': 4786, 'hereas': 4787, 'event': 4788, 'held': 4789, 'pissd': 4790, 'surprise': 4791, 'golden': 4792, 'hopes': 4793, 'bruschetta': 4794, 'devine': 4795, 'employee': 4796, 'lastly': 4797, 'mozzarella': 4798, 'negligent': 4799, 'unwelcome': 4800, 'seasonal': 4801, 'fruit': 4802, 'peach': 4803, 'officially': 4804, 'containers': 4805, 'opposed': 4806, 'cramming': 4807, 'takeout': 4808, 'boxes': 4809, 'crpe': 4810, 'delicate': 4811, 'kabuki': 4812, 'maria': 4813, 'article': 4814, 'spices': 4815, 'fucking': 4816, 'caballero': 4817, 'oysters': 4818, 'qualified': 4819, 'foods': 4820, 'tolerance': 4821, 'polite': 4822, 'wash': 4823, 'biscuit': 4824, 'coconut': 4825, 'fella': 4826, 'huevos': 4827, 'rancheros': 4828, 'wines': 4829, 'pricey': 4830, 'temp': 4831, 'prepare': 4832, 'bare': 4833, 'gloves': 4834, 'oil': 4835, 'pleasure': 4836, 'plethora': 4837, 'sandwiches': 4838, 'seal': 4839, 'approval': 4840, 'college': 4841, 'cooking': 4842, 'besides': 4843, 'costco': 4844, 'highlighted': 4845, 'grocery': 4846, 'dude': 4847, 'doughy': 4848, 'inch': 4849, 'albondigas': 4850, 'tomato': 4851, 'meatballs': 4852, 'occasions': 4853, 'medium': 4854, 'bloodiest': 4855, 'anymore': 4856, 'chai': 4857, 'latte': 4858, 'allergy': 4859, 'warnings': 4860, 'clue': 4861, 'contain': 4862, 'peanuts': 4863, 'mediterranean': 4864, 'beers': 4865, 'highlight': 4866, 'concern': 4867, 'mellow': 4868, 'mushroom': 4869, 'strawberry': 4870, 'unprofessional': 4871, 'loyal': 4872, 'patron': 4873, 'occasional': 4874, 'pats': 4875, 'bellagio': 4876, 'anticipated': 4877, 'sals': 4878, 'fav': 4879, 'unexperienced': 4880, 'employees': 4881, 'chickens': 4882, 'steakhouse': 4883, 'concept': 4884, 'guacamole': 4885, 'pured': 4886, 'postinos': 4887, 'poisoning': 4888, 'batch': 4889, 'yay': 4890, 'eve': 4891, 'caring': 4892, 'teamwork': 4893, 'degree': 4894, 'ri': 4895, 'calamari': 4896, 'fondue': 4897, 'denny': 4898, 'downright': 4899, 'waaaaaayyyyyyyyyy': 4900, 'sangria': 4901, 'glass': 4902, 'brisket': 4903, 'trippy': 4904, 'hurry': 4905, 'reservation': 4906, 'stretch': 4907, 'cashew': 4908, 'chipolte': 4909, 'ranch': 4910, 'dipping': 4911, 'sause': 4912, 'watered': 4913, 'workers': 4914, 'douchey': 4915, 'garden': 4916, 'con': 4917, 'spotty': 4918, 'ensued': 4919, 'apologize': 4920, 'binge': 4921, 'drinking': 4922, 'carbs': 4923, 'insults': 4924, 'profound': 4925, 'deuchebaggery': 4926, 'solidify': 4927, 'combo': 4928, 'ala': 4929, 'cart': 4930, 'blame': 4931, 'rave': 4932, 'del': 4933, 'hamburger': 4934, 'ya': 4935, 'fireball': 4936, 'disapppointment': 4937, 'correction': 4938, 'heimer': 4939, 'brownish': 4940, 'ha': 4941, 'flop': 4942, 'bigger': 4943, 'unwrapped': 4944, 'mile': 4945, 'brushfire': 4946, 'mirage': 4947, 'refried': 4948, 'dried': 4949, 'crusty': 4950, 'caterpillar': 4951, 'appetite': 4952, 'instantly': 4953, 'ninja': 4954, 'poured': 4955, 'wound': 4956, 'drawing': 4957}\n"
     ]
    }
   ],
   "source": [
    "word_dict = tokenizer.word_index\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in each entry of the reviews column can be transformed to its numeric mapping using the one_hot function from keras.  The pad_sequences function can then be applied to ensure each entry has the same length by padding with 0's at the end.  The input to the RNN will need to be further transformed into a more generalized type of vector called a tensor.  Word embedding length refers to tokenized word positions within a tensor and can be calculated as the 4th root of vocabulary size.  It will be necessary to specify this length in the first layer of the network.   Since the input to a NN model will be a tensor, each element is required to have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4097, 1081, 1292, 604, 2008, 775]\n"
     ]
    }
   ],
   "source": [
    "# transform reviews by numeric mappings\n",
    "rev_encoded = [keras.preprocessing.text.one_hot(w, len(vocab)) for w in rev_list]\n",
    "print(rev_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad encoded reviews with 0's at the end of each vector\n",
    "padded_revs = pad_sequences(rev_encoded, maxlen = max_rev_len, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4097, 1081, 1292,  604, 2008,  775,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_revs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the NN will be a dense layer with 2 sentiment categories - 1 for positive, 0 for negative.  A sigmoid activation function can be used when the outcome is binary, however this function does not normalize the probabilites.  To obtain normalized probabilities in the output that sum to 1, the softmax function can be used.  Softmax is often used for multi-classification predictions, but inherently ensures normalization and so will be what is used on the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been read from all three sources, combined, and preprocessed.  Preprocessing involved removing special characters so that only words consisting of lower-case letters remained.  Stop words were filtered out, and the remaining words were lemmatized so that there were no variations of the same word stems remaining.  (The code was adapdted from the Stack Overflow entry on removing stopwords.) The words in each review were codified by a numeric dictionary and the size of each review was made the same as the maximum length through padding.  Before using the processed data, it needs to be split for training, validation and testing.  A common practice is to use 50% of the data for training and 25% for each validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_revs, sent_df['sentiment'], test_size=0.25, stratify = sent_df['sentiment'])\n",
    "# split 75% of above train data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train vocab size:  1545\n"
     ]
    }
   ],
   "source": [
    "train_vocab_size = len(X_train)\n",
    "print(\"Train vocab size: \", train_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding length:  6\n"
     ]
    }
   ],
   "source": [
    "# word embedding length\n",
    "train_emb = int(round(np.sqrt(np.sqrt(train_vocab_size)),0))\n",
    "print(\"Word embedding length: \", train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final train, validation and testing sets as csvs\n",
    "pd.DataFrame(X_train).to_csv('X_training.csv')\n",
    "pd.DataFrame(X_val).to_csv('X_validation.csv')\n",
    "pd.DataFrame(X_test).to_csv('X_testing.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_training.csv')\n",
    "pd.DataFrame(y_val).to_csv('y_validation.csv')\n",
    "pd.DataFrame(y_test).to_csv('y_testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequential recurrent neural network (RNN) architecture was used.  Keras allows such a model to be built by adding successive layers of nodes.  Weights for each node are computed using activation functions as data moves forward through the network.  Back propagation traverses the network in the opposite direction to update weights by minimizing a loss function.  The minimizing step involves differentiation, or calculating a gradient.  A node could be dropped when it maybe shouldn't if the gradient becomes too close to 0.  Other issues arise when the gradient becomes too large, approaching infinity.  A long short term memory (LSTM) variant of RNN can mitigate these gradient issues.  (DataCamp.  Recurrent Nerual Networks for Language Modelling in Python.)\n",
    "\n",
    "Embedding basically converts the padded input into a matrix of word vectors, with each vector containing numerically transformed words of similar meaning. (Saxena, 2020) This is to reduce the size of input.  However, to add a dense layer the matrix needs to be \"flattened\" into one dimension. (What is the role of \"Flatten in Keras?) Adding 2 LSTM layers can take care of this, while also using back propagation to optimize node weights.  A dropout layer can be used to drop some neurons and avoid overfitting. Otherwise dropout can be specified in LSTM.  Recurrent dropout has more to do with keeping nodes during CPU/GPU cycles than it does amongst network layers.  A common proportion of node input to be dropped is 0.2; anything less may not have an effect on reducing overfitting.  (Sarma, 2020)  Recurrent dropout was set to less than dropout so that a small number of nodes will be dropped regardless of layer, which adds extra protection to diminishing gradient issues.  Another option is to use GlobalAveragePooling which can flatten an embedded layer, reduce overfitting, and speed up training the model by applying averaging on the input vectors from the previous layer.  (Adventures in Machine Learning)\n",
    "\n",
    "A Flatten() layer was tried after the ebmedding layer, as was an LSTM layer with 128, and 64 nodes.  After each post-embedding layer, one then 2 dense hidden layers were tried with varying numbers of nodes.  After at most 30 epochs the best training and validation accuracies found barely exceeded 0.6.  The GolbalAveragePooling layer in the model presented below finally resulted in accuracies and losses that looked much better.  Also, each epoch of fitting with the models that used LSTM took several minutes.  This became very aggravating having to wait so long just to experiment with over 20 or more epochs.  The settled on model using GlobalAveragPooling crunched through 100 epochs in less than 1 minute.  \n",
    "\n",
    "Specifying the input dimensions is only required for the first layer in a sequential model.  Each vector in the embedding layer is of length 8, so the number of parameters is that multiplied by vocabulary size of 4958, which equals 39,664.  The same number of nodes carried through the GlobalAveragePooling and dropout layers and no parameters were specified.  The number of parameters in the dense layers can be calculated as the product of number of nodes for current and previous layer plus nodes for current layer.  As mentioned above, 100 nodes was settled on by trial and error and resulted in 100 + 8x100 = 900 parameters.  The second layer resulted in 50 + 50x100 = 5050 parameters.  The predictions of the neural net will be 1 or 0, so only 1 node is desired for the final layer giving it 1 + 50x1 = 51 parameters.  \n",
    "\n",
    "In order to capture non-linearities between node values an activation function can be used on layers.  The Rectified Linear Unit, or Relu, function was used for the first 2 dense layers.  Relu evaluates to 0 when input is less than 0, otherwise it returns the input value itself.  Relu has been shown to work very efficiently in neural networks compared to other activation functions such as tanh.  (DataCamp.  Introduction to Deep Learning in Python.)  The final dense layer used the sigmoid activation function which is useful for making binary predictions.  \n",
    "\n",
    "When compiling the model an optimizer and loss function can be specified.  The optimizer controls learning rate, while loss function minimizes error.  The Adam optimizer is a versatile algorithm that adjusts learning rate while performing gradient descent during back-propagation.  (DataCamp. Introduction to Deep Learning in Python.)  The binary-crossentropy loss function was chosen because the final outcome has 2 possible values.  The evaluation metric is accuracy, which is common for a classification problem.   \n",
    "\n",
    "Updating node weights is often performed on varying subsets of data until all weights have been updated.  This process can be performed again using different subsets and is called an epoch.  If more epochs are used the hope is that model performance metric will improve.  After a certain number of epochs the model may fit better to the training data and cause overfitting.  An early stopping point would be warranted to limit the number of epochs so that this doesn't occur.  Patience is used to specify how many succesive epochs to compare before stopping.  Typically a patience of 2 or 3 is enough.  Just in case the performance begain to diverge again during epoch fittings, I thought that using 3 may be wiser than 2 for patience when applying early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 677, 8)            39664     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_20  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 45,665\n",
      "Trainable params: 45,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emb, input_length=max_rev_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 5ms/step - loss: 0.6938 - accuracy: 0.4919 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5036\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4887 - val_loss: 0.6932 - val_accuracy: 0.4964\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5061 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4971 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4828 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.5256 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6929 - val_accuracy: 0.5036\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.4997 - val_loss: 0.6929 - val_accuracy: 0.5036\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5074 - val_loss: 0.6928 - val_accuracy: 0.5036\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5068 - val_loss: 0.6923 - val_accuracy: 0.5036\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.4945 - val_loss: 0.6921 - val_accuracy: 0.4964\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5631 - val_loss: 0.6911 - val_accuracy: 0.6827\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6886 - accuracy: 0.5540 - val_loss: 0.6887 - val_accuracy: 0.5983\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.6058 - val_loss: 0.6818 - val_accuracy: 0.7496\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6638 - accuracy: 0.6414 - val_loss: 0.6744 - val_accuracy: 0.5051\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6401 - accuracy: 0.6388 - val_loss: 0.7217 - val_accuracy: 0.5036\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.5846 - accuracy: 0.7256 - val_loss: 0.6059 - val_accuracy: 0.6638\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.4959 - accuracy: 0.7825 - val_loss: 0.5535 - val_accuracy: 0.7205\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.4038 - accuracy: 0.8492 - val_loss: 0.5187 - val_accuracy: 0.7453\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8828 - val_loss: 0.4950 - val_accuracy: 0.7555\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.3097 - accuracy: 0.8777 - val_loss: 0.4985 - val_accuracy: 0.7555\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8958 - val_loss: 0.6477 - val_accuracy: 0.6870\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2677 - accuracy: 0.8951 - val_loss: 0.5326 - val_accuracy: 0.7482\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2390 - accuracy: 0.9003 - val_loss: 0.5134 - val_accuracy: 0.7540\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2447 - accuracy: 0.8977 - val_loss: 0.5256 - val_accuracy: 0.7555\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2220 - accuracy: 0.9074 - val_loss: 0.6547 - val_accuracy: 0.7249\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2013 - accuracy: 0.9256 - val_loss: 0.5415 - val_accuracy: 0.7613\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.9178 - val_loss: 0.5998 - val_accuracy: 0.7424\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9184 - val_loss: 0.5835 - val_accuracy: 0.7511\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9288 - val_loss: 0.7664 - val_accuracy: 0.7001\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2486 - accuracy: 0.8932 - val_loss: 0.5757 - val_accuracy: 0.7598\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1774 - accuracy: 0.9346 - val_loss: 0.6119 - val_accuracy: 0.7453\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9301 - val_loss: 0.5821 - val_accuracy: 0.7642\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9204 - val_loss: 0.6001 - val_accuracy: 0.7627\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9333 - val_loss: 0.6495 - val_accuracy: 0.7380\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9385 - val_loss: 0.6487 - val_accuracy: 0.7424\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9366 - val_loss: 0.6233 - val_accuracy: 0.7511\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.9437 - val_loss: 0.6246 - val_accuracy: 0.7642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1504 - accuracy: 0.9456 - val_loss: 0.6585 - val_accuracy: 0.7540\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9463 - val_loss: 0.6345 - val_accuracy: 0.7642\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1417 - accuracy: 0.9463 - val_loss: 0.6642 - val_accuracy: 0.7467\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9392 - val_loss: 0.6523 - val_accuracy: 0.7642\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9430 - val_loss: 0.7006 - val_accuracy: 0.7380\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9469 - val_loss: 0.6728 - val_accuracy: 0.7598\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9521 - val_loss: 0.7120 - val_accuracy: 0.7467\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9417 - val_loss: 0.6773 - val_accuracy: 0.7627\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1405 - accuracy: 0.9430 - val_loss: 0.6987 - val_accuracy: 0.7584\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9437 - val_loss: 0.6864 - val_accuracy: 0.7656\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9405 - val_loss: 0.7122 - val_accuracy: 0.7409\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.9592 - val_loss: 0.7000 - val_accuracy: 0.7671\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9566 - val_loss: 0.7021 - val_accuracy: 0.7584\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9579 - val_loss: 0.7088 - val_accuracy: 0.7598\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9534 - val_loss: 0.8492 - val_accuracy: 0.7191\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1480 - accuracy: 0.9424 - val_loss: 0.7341 - val_accuracy: 0.7482\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.9586 - val_loss: 0.7407 - val_accuracy: 0.7453\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1193 - accuracy: 0.9586 - val_loss: 0.7337 - val_accuracy: 0.7482\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.9612 - val_loss: 0.7563 - val_accuracy: 0.7424\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9521 - val_loss: 0.7874 - val_accuracy: 0.7424\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9579 - val_loss: 0.7431 - val_accuracy: 0.7613\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1301 - accuracy: 0.9476 - val_loss: 0.9465 - val_accuracy: 0.7263\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9508 - val_loss: 0.7689 - val_accuracy: 0.7569\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9631 - val_loss: 0.8059 - val_accuracy: 0.7409\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.9566 - val_loss: 0.7789 - val_accuracy: 0.7569\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9625 - val_loss: 0.7794 - val_accuracy: 0.7555\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9625 - val_loss: 0.7866 - val_accuracy: 0.7438\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1086 - accuracy: 0.9599 - val_loss: 0.8099 - val_accuracy: 0.7424\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.9605 - val_loss: 0.8839 - val_accuracy: 0.7249\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9638 - val_loss: 0.8433 - val_accuracy: 0.7424\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9573 - val_loss: 0.8194 - val_accuracy: 0.7525\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9638 - val_loss: 0.8084 - val_accuracy: 0.7569\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9715 - val_loss: 0.9508 - val_accuracy: 0.7191\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9605 - val_loss: 0.9026 - val_accuracy: 0.7394\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9605 - val_loss: 0.8192 - val_accuracy: 0.7555\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9676 - val_loss: 0.8585 - val_accuracy: 0.7438\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9670 - val_loss: 0.8540 - val_accuracy: 0.7409\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9735 - val_loss: 0.8475 - val_accuracy: 0.7482\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9702 - val_loss: 0.8501 - val_accuracy: 0.7482\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9715 - val_loss: 0.8577 - val_accuracy: 0.7496\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9722 - val_loss: 0.8766 - val_accuracy: 0.7482\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9715 - val_loss: 0.9504 - val_accuracy: 0.7336\n"
     ]
    }
   ],
   "source": [
    "no_stop = model.fit(X_train, y_train, epochs = 100, validation_data =(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 5ms/step - loss: 0.6934 - accuracy: 0.4893 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5042 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.4731 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5055 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.4777 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6934 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.5036\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.4939 - val_loss: 0.6932 - val_accuracy: 0.5036\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5049 - val_loss: 0.6930 - val_accuracy: 0.5036\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5167\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5049 - val_loss: 0.6929 - val_accuracy: 0.5036\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5049 - val_loss: 0.6927 - val_accuracy: 0.5036\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5055 - val_loss: 0.6925 - val_accuracy: 0.6477\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5036 - val_loss: 0.6932 - val_accuracy: 0.4964\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.4906 - val_loss: 0.6924 - val_accuracy: 0.5051\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5327 - val_loss: 0.6919 - val_accuracy: 0.4964\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5437 - val_loss: 0.6990 - val_accuracy: 0.5036\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5696 - val_loss: 0.6864 - val_accuracy: 0.5036\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.6408 - val_loss: 0.6738 - val_accuracy: 0.5983\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6460 - accuracy: 0.6589 - val_loss: 0.6472 - val_accuracy: 0.7336\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.5861 - accuracy: 0.7353 - val_loss: 0.6105 - val_accuracy: 0.6521\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.4855 - accuracy: 0.8265 - val_loss: 0.5398 - val_accuracy: 0.7569\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.4170 - accuracy: 0.8388 - val_loss: 0.5737 - val_accuracy: 0.6769\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3461 - accuracy: 0.8680 - val_loss: 0.5397 - val_accuracy: 0.7249\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2947 - accuracy: 0.8939 - val_loss: 0.4935 - val_accuracy: 0.7598\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2744 - accuracy: 0.8971 - val_loss: 0.5318 - val_accuracy: 0.7409\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2735 - accuracy: 0.8893 - val_loss: 0.5053 - val_accuracy: 0.7569\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2323 - accuracy: 0.9197 - val_loss: 0.5194 - val_accuracy: 0.7613\n"
     ]
    }
   ],
   "source": [
    "# early stopping\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "early_stop = model.fit(X_train, y_train, epochs = 100, callbacks=[early_stopping_monitor], verbose=True, validation_data =(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 epochs of model fitting were tried to start with.  Eventually the training accuracy reached a high point then wavered, as did validation accuracy.  When training accuracy continues to increase, but validation accuracy goes down overfitting is likely beggining to occur.  The model was compiled once again and fitted using early stopping to avoid these overfitted epochs.  Graphical depections of accuracies and losses over epochs may make this easier to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracies\n",
    "plt.plot(early_stop.history['accuracy'], 'b', label='Training Accuracy')\n",
    "plt.plot(early_stop.history['val_accuracy'], 'r', label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(early_stop.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(early_stop.history['val_loss'], 'r', label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5641 - accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on validation set\n",
    "val_acc = model.evaluate(X_val, y_val)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss eventually became very low, which is good, however when evaluating on the holdout validation set the loss grew significantly.  The validation accuracy was very close to the testing accuracy which indicates the model would continue to make fairly decent predictions on future data.  Test and validation accuracies closer to the much higher training accuracies would be more desirable in order to say that the model would continue to make good predictions.  The fact that this happened indicates there is still some overfitting going on, and there remains room for improvement.\n",
    "\n",
    "A wiser method of choosing nodes in each layer could be used instead of trial and error.  Grid search parameter optimization could be deployed along with more dense layers.  It would also be interesting to see if each of the 3 individual data sets resulted in differing performance.  Perhaps this could capture some subtle indication of differing voices amongst platform users.  Another thing to ponder is whether removing stop words and lemmatizing took away too much sentiment in the reviews.  Perhaps a model would perform better if more raw words were kept.  Either way 76% accuracy in prediction is a promising indication that, indeed, positive and negative customer sentiment can be extracted through natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "\n",
    "DataCamp. (n.d.) Recurrent Nerual Networks for Language Modelling in Python.\n",
    "\n",
    "\n",
    "DataCamp. (n.d.) Introduction to Deep Learning in Python.\n",
    "\n",
    "\n",
    "An Introduction to Global Average Pooling in Convolutional Neural Networks.  (2019, May 23).  Adventures in Machine Learning.  http://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/\n",
    "\n",
    "\n",
    "Python remove stop words from pandas dataframe. (n.d) Stack Overflow. https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "\n",
    "\n",
    "Saxena, Sawan. (2020, Oct 3.)  Understanding Embedding Layer in Keras.  Analytics Vidhya. https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce\n",
    "\n",
    "\n",
    "What is the role of \"Flatten\" in Keras? (n.d.) Stack Overflow. https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
    "\n",
    "\n",
    "Sarma, Palash.  (2020, Oct 25).  Keras Dropout Layer Explained for Beginners.  MachineLearning Kowledge.ai.  https://machinelearningknowledge.ai/keras-dropout-layer-explained-for-beginners/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
